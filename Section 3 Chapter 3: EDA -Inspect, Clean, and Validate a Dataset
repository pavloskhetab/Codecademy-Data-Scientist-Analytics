EDA: Inspect, Clean, and Validate a Dataset
Learn how EDA can inform the data cleaning process.

Introduction
One of the most challenging parts of data cleaning is diagnosing data issues and figuring out HOW to most effectively address them. 
In order to accomplish this, exploratory data analysis (EDA) can be an extremely useful tool. In this article, we’ll walk through 
an example dataset to demonstrate how EDA can inform the initial data inspection, cleaning, and validation process.

While this article serves as an introduction to EDA for data cleaning, it is important to note that every dataset is different, 
and therefore will require different exploration. EDA is all about following the data, verifying your assumptions, and investigating anything that is unexpected.

Initial Data Inspection
Before analysis or cleaning, it is useful to print a few rows of data. This helps ensure that the data is properly loaded. 
It also allows us to compare the observed data to the data dictionary and determine whether the coding appears to match our expectations. 
For example, let’s load and inspect the first few rows of a dataset of heart disease patients (downloaded from the UCI Machine Learning Repository).

import pandas as pd
heart = pd.read_csv('processed.cleveland.data.csv')
print(heart.head())

Output:

First five rows of the heart disease data, which includes columns age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, 
slope, ca, thal, and heart_disease.

There are a few things we might want to inspect. For example, the data dictionary gives the following information about the cp column:

cp: chest pain type

Value 1: typical angina
Value 2: atypical angina
Value 3: non-anginal pain
Value 4: asymptomatic
Based on this information, it’s not necessarily clear whether the data is going to be coded as numerical values (eg., 1, 2, 3, or 4) or 
with strings (eg., 'typical angina'). Data inspection allows us to clarify that this column contains numerical values.

Similarly, there is some conflicting information in the data dictionary about the target column (note: we renamed this column as 
heart_disease before loading it, but it was originally coded as num). The list of features contains the following information about this column:

num: diagnosis of heart disease (angiographic disease status)

Value 0: < 50% diameter narrowing
Value 1: > 50% diameter narrowing
However, the initial data description suggests that the target field is integer valued from 0-4, where 0 indicates no heart disease, 
and values 1-4 indicate the presence of heart disease.

By inspecting the first few rows of data, we see at least one instance of the value 2 in the heart_disease column. 
This suggests that the values probably range from 0-4 instead of just 0-1. We could verify this with 
further exploration (e.g., by using heart.heart_disease.value_counts() to get a table of values in this column).

Data Information
Once we’ve taken a first look at some data, a common next step is to address questions such as:

How many (non-null) observations do we have?
How many unique columns/features do we have?
Which columns (if any) contain missing data?
What is the data type of each column?
Using pandas, we can easily address these questions using the .info() method. For example:

print(heart.info())

Output:

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 303 entries, 0 to 302
Data columns (total 14 columns):
 #   Column         Non-Null Count  Dtype  
---  ------         --------------  -----  
 0   age            303 non-null    float64
 1   sex            303 non-null    float64
 2   cp             303 non-null    float64
 3   trestbps       303 non-null    float64
 4   chol           303 non-null    float64
 5   fbs            303 non-null    float64
 6   restecg        303 non-null    float64
 7   thalach        303 non-null    float64
 8   exang          303 non-null    float64
 9   oldpeak        303 non-null    float64
 10  slope          303 non-null    float64
 11  ca             303 non-null    object 
 12  thal           303 non-null    object 
 13  heart_disease  303 non-null    int64  
dtypes: float64(11), int64(1), object(2)
memory usage: 33.3+ KB

There are a few interesting pieces of information that we can glean from this output:

There are 303 rows and 14 columns of data
At first glance, there are no null (i.e., missing) values in any column (we’ll come back to this)
The ca and thal columns have a data type of object (which suggests that they are strings), even though we saw in our initial inspection 
that these columns appear to contain numerical values
To investigate the unexpected output here, we might want to take a look at the unique values in the ca column:

print(heart.ca.unique())

Output:

array(['0.0', '3.0', '2.0', '1.0', '?'], dtype=object)

We note that at least one row contains a '?' in this column. We can probably assume that this indicates mis-coded missing data. 
The '?' also probably forced the column to be coded as a string because there is no obvious way to cast a '?' to a numerical value.

Given this information, we now have more to do! We can replace any instance of '?' with np.NaN, change the data type of this column back to a float or integer, 
and then re-print the heart.info() to determine how many missing values we’ve got. Then, we probably want to do a similar inspection of the thal column.

Inspecting Missing Data
After identifying that there is some missing data and converting it to a format that Python can recognize, it’s often a good idea to take a closer 
look at those rows. Sometimes, we can find clues as to WHY the data is missing, which can help us make decisions about whether to get rid of the rows 
altogether or impute the missing values somehow.

heart[heart.isnull().any(axis=1)]

Output:

table with 6 rows, four rows with missing data in the ca column and two with missing data in the thal column.

Looking at this output, we note that there is no overlap between the rows with missing ca data and missing thal data. 
This suggests that these patients are missing ca and thal information for different reasons. We don’t see any immediate clues as to why 
the data is missing in the first place, but we can inspect this further once we start digging into individual features.

Data Exploration in Real-Time
If you’d like to watch us inspect this dataset in real-time, feel free to checkout the livestream recording below:


If you’d like to play with the data yourself, you can download the code and data from our Github repository.

References
This data is downloaded from the UCI Machine Learning Repository:

Dua, D. and Graff, C. (2019). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science.

The principal investigators responsible for data collection were:

Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.
University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.
University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.
V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D
