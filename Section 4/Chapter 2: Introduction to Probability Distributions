Random Variables
A random variable is, in its simplest form, a function. In probability, we often use random variables to represent random events. For example, we could use a random variable to represent the outcome of a die roll: any number between one and six.

Random variables must be numeric, meaning they always take on a number rather than a characteristic or quality. If we want to use a random variable to represent an event with non-numeric outcomes, we can choose numbers to represent those outcomes. For example, we could represent a coin flip as a random variable by assigning “heads” a value of 1 and “tails” a value of 0.

In this lesson, we will use random.choice(a, size = size, replace = True/False) from the numpy library to simulate random variables in python. In this method:

a is a list or other object that has values we are sampling from
size is a number that represents how many values to choose
replace can be equal to True or False, and determines whether we keep a value in a after drawing it (replace = True) or remove it from the pool (replace = False).
The following code simulates the outcome of rolling a fair die twice using np.random.choice():

import numpy as np

# 7 is not included in the range function
die_6 = range(1, 7)

rolls = np.random.choice(die_6, size = 2, replace = True)

print(rolls)

Output:

# [2, 5]

Code: 
import numpy as np

# create 6 sided "die"
die_6 = range(1, 7)

# set number of rolls
num_rolls = 10

# roll the "die" the set amount of times
results_1 = np.random.choice(die_6, size = num_rolls, replace = True)
print(results_1)

# create 12-sided "die"
die_12 = range(1,13)

# roll the 12-sided "die" 10 times
results_2 = np.random.choice(die_12, size = num_rolls, replace = True)



Discrete and Continuous Random Variables
Discrete Random Variables
Random variables with a countable number of possible values are called discrete random variables. 
For example, rolling a regular 6-sided die would be considered a discrete random variable because the outcome options are limited to the numbers on the die.

Discrete random variables are also common when observing counting events, such as how many people entered a store on a randomly selected day. 
In this case, the values are countable in that they are limited to whole numbers (you can’t observe half of a person).

Continuous Random Variables
When the possible values of a random variable are uncountable, it is called a continuous random variable. 
These are generally measurement variables and are uncountable because measurements can always be more precise – meters, centimeters, millimeters, etc.

For example, the temperature in Los Angeles on a randomly chosen day is a continuous random variable. 
We can always be more precise about the temperature by expanding to another decimal place (96 degrees, 96.44 degrees, 96.437 degrees, etc.).

Code:
mile_time = 'continuous'

candy = 'discrete'



Probability Mass Functions
A probability mass function (PMF) is a type of probability distribution that defines the probability of observing a particular value of a discrete random variable. 
For example, a PMF can be used to calculate the probability of rolling a three on a fair six-sided die.

There are certain kinds of random variables (and associated probability distributions) that are relevant for many different kinds of problems. 
These commonly used probability distributions have names and parameters that make them adaptable for different situations.

For example, suppose that we flip a fair coin some number of times and count the number of heads. 
The probability mass function that describes the likelihood of each possible outcome (eg., 0 heads, 1 head, 2 heads, etc.) is called the binomial distribution. The parameters for the binomial distribution are:

n for the number of trials (eg., n=10 if we flip a coin 10 times)
p for the probability of success in each trial (probability of observing a particular outcome in each trial. In this example, p= 0.5 because the probability of observing heads on a fair coin flip is 0.5)
If we flip a fair coin 10 times, we say that the number of observed heads follows a Binomial(n=10, p=0.5) distribution. The graph below shows the probability mass function for this experiment. 
The heights of the bars represent the probability of observing each possible outcome as calculated by the PMF.

Code:
See how the shape of the binomial distribution changes as the sample size changes.

The heights of the bars represent the probability of observing different values of heads from x number of fair coin flips. Taller bars represent more likely outcomes. For example, let’s say we flip a fair coin 5 times, and want to know the probability of getting between 1 and 3 heads. We can visualize this scenario with the probability mass function shown.

We can calculate this using the following equation where P(x) is the probability of observing the number x successes (heads in this case):

P(1to3heads)=P(1<=X<=3)
P(1to3heads)=P(X=1)+P(X=2)+P(X=3)
P(1to3heads)=0.1562+0.3125+0.3125
P(1to3heads)=0.7812
​
 
The sum of the heights of all the bars will always equal 1. So when x is larger, the number of heads we can observe increases, and the probability needs to be divided between more values.



Calculating Probabilities using Python
The binom.pmf() method from the scipy.stats library can be used to calculate the PMF of the binomial distribution at any value. This method takes 3 values:

x: the value of interest
n: the number of trials
p: the probability of success
For example, suppose we flip a fair coin 10 times and count the number of heads. 
We can use the binom.pmf() function to calculate the probability of observing 6 heads as follows:

# import necessary library
import scipy.stats as stats

# st.binom.pmf(x, n, p)
print(stats.binom.pmf(6, 10, 0.5))

Output:

# 0.205078

Notice that two of the three values that go into the stats.binomial.pmf() method are the parameters that define the binomial distribution: 
n represents the number of trials and p represents the probability of success.

Code:
import scipy.stats as stats

# value of interest
# change this
x = 3

# sample size
# change this
n = 10

# calculate probability
prob_1 = stats.binom.pmf(x, n, 0.5)
print(prob_1)

## Question 2
prob_2 = stats.binom.pmf(7, 20, 0.5)
print(prob_2)



Using the Probability Mass Function Over a Range
We have seen that we can calculate the probability of observing a specific value using a probability mass function. 
What if we want to find the probability of observing a range of values for a discrete random variable? One way we could do this is by adding up the probability of each value.

For example, let’s say we flip a fair coin 5 times, and want to know the probability of getting between 1 and 3 heads. We can visualize this scenario with the probability mass function:

GIF of highlighting selected bars on a histogram

We can calculate this using the following equation where P(x) is the probability of observing the number x successes (heads in this case):

P(1to3heads)=P(1<=X<=3)
P(1to3heads)=P(X=1)+P(X=2)+P(X=3)
P(1to3heads)=0.1562+0.3125+0.3125
P(1to3heads)=0.7812
​
 
Instructions
Let’s visualize what it means to take the probability of a range.

Try different ranges to see how the probabilities change for different values.

We can calculate this using the following equation where P(x) is the probability of observing the number of heads in this case:

P(1to3heads)=P(1<=X<=3)
P(1to3heads)=P(X=1)+P(X=2)+P(X=3)
P(1to3heads)=0.1562+0.3125+0.3125
P(1to3heads)=0.7812
​


Probability Mass Function Over a Range using Python
We can use the same binom.pmf() method from the scipy.stats library to calculate the probability of observing a range of values. 
As mentioned in a previous exercise, the binom.pmf method takes 3 values:

x: the value of interest
n: the sample size
p: the probability of success
For example, we can calculate the probability of observing between 2 and 4 heads from 10 coin flips as follows:

import scipy.stats as stats

# calculating P(2-4 heads) = P(2 heads) + P(3 heads) + P(4 heads) for flipping a coin 10 times
print(stats.binom.pmf(2, n=10, p=.5) + stats.binom.pmf(3, n=10, p=.5) + stats.binom.pmf(4, n=10, p=.5))

Output:

# 0.366211

We can also calculate the probability of observing less than a certain value, let’s say 3 heads, by adding up the probabilities of the values below it:

import scipy.stats as stats

# calculating P(less than 3 heads) = P(0 heads) + P(1 head) + P(2 heads) for flipping a coin 10 times
print(stats.binom.pmf(0, n=10, p=.5) + stats.binom.pmf(1, n=10, p=.5) + stats.binom.pmf(2, n=10, p=.5))

Output:

# 0.0546875

Note that because our desired range is less than 3 heads, we do not include that value in the summation.

When there are many possible values of interest, this task of adding up probabilities can be difficult. 
If we want to know the probability of observing 8 or fewer heads from 10 coin flips, we need to add up the values from 0 to 8:

import scipy.stats as stats

stats.binom.pmf(0, n = 10, p = 0.5) + stats.binom.pmf(1, n = 10, p = 0.5) + stats.binom.pmf(2, n = 10, p = 0.5) + stats.binom.pmf(3, n = 10, p = 0.5) + stats.binom.pmf(4, n = 10, p = 0.5) + stats.binom.pmf(5, n = 10, p = 0.5) + stats.binom.pmf(6, n = 10, p = 0.5) + stats.binom.pmf(7, n = 10, p = 0.5) + stats.binom.pmf(8, n = 10, p = 0.5)

Output:

# 0.98926

This involves a lot of repetitive code. Instead, we can also use the fact that the sum of the probabilities for all possible values is equal to 1:

P(0to8heads)+P(9to10heads)=P(0to10heads)=1
P(0to8heads)=1−P(9to10heads)
​
 
Now instead of summing up 9 values for the probabilities between 0 and 8 heads, we can do 1 minus the sum of two values and get the same result:

import scipy.stats as stats
# less than or equal to 8
1 - (stats.binom.pmf(9, n=10, p=.5) + stats.binom.pmf(10, n=10, p=.5))

Output:

# 0.98926

Code:
import scipy.stats as stats

## Checkpoint 1
prob_1 = 0
for x in range(4,7):
  prob_1 += stats.binom.pmf(x, n = 10, p = 0.5)

print(prob_1)

## Checkpoint 2
prob_2 = 0
for x in range(0,3):
  prob_2 += stats.binom.pmf(x, n = 10, p = 0.5)

prob_2 = 1 - prob_2
print(prob_2)



Cumulative Distribution Function
The cumulative distribution function for a discrete random variable can be derived from the probability mass function. 
However, instead of the probability of observing a specific value, the cumulative distribution function gives the probability of observing a specific value OR LESS.

As previously discussed, the probabilities for all possible values in a given probability distribution add up to 1. 
The value of a cumulative distribution function at a given value is equal to the sum of the probabilities lower than it, with a value of 1 for the largest possible number.

Cumulative distribution functions are constantly increasing, so for two different numbers that the random variable could take on, 
the value of the function will always be greater for the larger number. Mathematically, this is represented as:

If x_1 < x_2 , →CDF(x_1)<CDF(x_2)

We showed how the probability mass function can be used to calculate the probability of observing less than 3 heads out of 10 coin flips by adding up the probabilities of observing 0, 1, and 2 heads. 
The cumulative distribution function produces the same answer by evaluating the function at CDF(X=2). In this case, using the CDF is simpler than the PMF because it requires one calculation rather than three.

The animation to the right shows the relationship between the probability mass function and the cumulative distribution function. 
The top plot is the PMF, while the bottom plot is the corresponding CDF. 
When looking at the graph of a CDF, each y-axis value is the sum of the probabilities less than or equal to it in the PMF.



Cumulative Distribution Function continued
We can use a cumulative distribution function to calculate the probability of a specific range by taking the difference between two values from the cumulative distribution function. 
For example, to find the probability of observing between 3 and 6 heads, we can take the probability of observing 6 or fewer heads and subtracting the probability of observing 2 or fewer heads. 
This leaves a remnant of between 3 and 6 heads.

The visual to the right demonstrates how this works. It is important to note that to include the lower bound in the range, the value being subtracted should be one less than the lower bound. 
In this example, we wanted to know the probability from 3 to 6, which includes 3. Mathematically, this looks like the following equation:

P(3<=X<=6)=P(X<=6)−P(X<3)
or
P(3<=X<=6)=P(X<=6)−P(X<=2)
​


Using the Cumulative Distribution Function in Python
We can use the binom.cdf() method from the scipy.stats library to calculate the cumulative distribution function. This method takes 3 values:

x: the value of interest, looking for the probability of this value or less
n: the sample size
p: the probability of success
Calculating the probability of observing 6 or fewer heads from 10 fair coin flips (0 to 6 heads) mathematically looks like the following:

P(6orfewerheads)=P(0to6heads)
In python, we use the following code:

import scipy.stats as stats

print(stats.binom.cdf(6, 10, 0.5))

Output:

0.828125

Calculating the probability of observing between 4 and 8 heads from 10 fair coin flips can be thought of as taking the difference of the value of the cumulative distribution function 
at 8 from the cumulative distribution function at 3:

P(4to8Heads)=P(0to8Heads)−P(0to3Heads)
In python, we use the following code:

import scipy.stats as stats

print(stats.binom.cdf(8, 10, 0.5) - stats.binom.cdf(3, 10, 0.5))

Output:

# 0.81738

To calculate the probability of observing more than 6 heads from 10 fair coin flips we subtract the value of the cumulative distribution function at 6 from 1. Mathematically, this looks like the following:

P(morethan6heads)=1−P(6orfewerheads)
Note that “more than 6 heads” does not include 6. In python, we would calculate this probability using the following code:

import scipy.stats as stats
print(1 - stats.binom.cdf(6, 10, 0.5))

Output:

# 0.171875

Code:
import scipy.stats as stats

## Checkpoint 1
prob_1 = stats.binom.cdf(3, 10, 0.5)
print(prob_1)

# compare to pmf code (checkpoint 2)
print(stats.binom.pmf(0, n=10, p=.5) + stats.binom.pmf(1, n=10, p=.5) + stats.binom.pmf(2, n=10, p=.5) + stats.binom.pmf(3, n=10, p=.5))


## Checkpoint 3
prob_2 = 1 - stats.binom.cdf(5, 10, 0.5)
print(prob_2)


## Checkpoint 4
prob_3 = stats.binom.cdf(5,10,0.5) - stats.binom.cdf(1,10,0.5)
print(prob_3)

# compare to pmf code (checkpoint 5)
print(stats.binom.pmf(2, n=10, p=.5) + stats.binom.pmf(3, n=10, p=.5) + stats.binom.pmf(4, n=10, p=.5) + stats.binom.pmf(5, n=10, p=.5))



Probability Density Functions
Similar to how discrete random variables relate to probability mass functions, continuous random variables relate to probability density functions. 
They define the probability distributions of continuous random variables and span across all possible values that the given random variable can take on.

When graphed, a probability density function is a curve across all possible values the random variable can take on, and the total area under this curve adds up to 1.

The following image shows a probability density function. The highlighted area represents the probability of observing a value within the highlighted range.

GIF or visual of the area under the curve highlighted and showing the calculated area under the curve

In a probability density function, we cannot calculate the probability at a single point. This is because the area of the curve underneath a single point is always zero. 
The gif below showcases this.

GIF or visual of the highlighted area under the curve getting smaller and smaller until the area equals 0

As we can see from the visual above, as the interval becomes smaller, the width of the area under the curve becomes smaller as well. 
When trying to evaluate the area under the curve at a specific point, the width of that area becomes 0, and therefore the probability equals 0.

We can calculate the area under the curve using the cumulative distribution function for the given probability distribution.

For example, heights fall under a type of probability distribution called a normal distribution. 
The parameters for the normal distribution are the mean and the standard deviation, and we use the form Normal(mean, standard deviation) as shorthand.

We know that women’s heights have a mean of 167.64 cm with a standard deviation of 8 cm, which makes them fall under the Normal(167.64, 8) distribution.

Let’s say we want to know the probability that a randomly chosen woman is less than 158 cm tall. 
We can use the cumulative distribution function to calculate the area under the probability density function curve from 0 to 158 to find that probability.

Image to show the area under the curve highlighted from 0 to 158 cm

We can calculate the area of the blue region in Python using the norm.cdf() method from the scipy.stats library. This method takes on 3 values:

x: the value of interest
loc: the mean of the probability distribution
scale: the standard deviation of the probability distribution
import scipy.stats as stats

# stats.norm.cdf(x, loc, scale)
print(stats.norm.cdf(158, 167.64, 8))

Output:

# 0.1141



