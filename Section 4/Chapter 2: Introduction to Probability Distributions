Random Variables
A random variable is, in its simplest form, a function. In probability, we often use random variables to represent random events. For example, we could use a random variable to represent the outcome of a die roll: any number between one and six.

Random variables must be numeric, meaning they always take on a number rather than a characteristic or quality. If we want to use a random variable to represent an event with non-numeric outcomes, we can choose numbers to represent those outcomes. For example, we could represent a coin flip as a random variable by assigning “heads” a value of 1 and “tails” a value of 0.

In this lesson, we will use random.choice(a, size = size, replace = True/False) from the numpy library to simulate random variables in python. In this method:

a is a list or other object that has values we are sampling from
size is a number that represents how many values to choose
replace can be equal to True or False, and determines whether we keep a value in a after drawing it (replace = True) or remove it from the pool (replace = False).
The following code simulates the outcome of rolling a fair die twice using np.random.choice():

import numpy as np

# 7 is not included in the range function
die_6 = range(1, 7)

rolls = np.random.choice(die_6, size = 2, replace = True)

print(rolls)

Output:

# [2, 5]

Code: 
import numpy as np

# create 6 sided "die"
die_6 = range(1, 7)

# set number of rolls
num_rolls = 10

# roll the "die" the set amount of times
results_1 = np.random.choice(die_6, size = num_rolls, replace = True)
print(results_1)

# create 12-sided "die"
die_12 = range(1,13)

# roll the 12-sided "die" 10 times
results_2 = np.random.choice(die_12, size = num_rolls, replace = True)



Discrete and Continuous Random Variables
Discrete Random Variables
Random variables with a countable number of possible values are called discrete random variables. 
For example, rolling a regular 6-sided die would be considered a discrete random variable because the outcome options are limited to the numbers on the die.

Discrete random variables are also common when observing counting events, such as how many people entered a store on a randomly selected day. 
In this case, the values are countable in that they are limited to whole numbers (you can’t observe half of a person).

Continuous Random Variables
When the possible values of a random variable are uncountable, it is called a continuous random variable. 
These are generally measurement variables and are uncountable because measurements can always be more precise – meters, centimeters, millimeters, etc.

For example, the temperature in Los Angeles on a randomly chosen day is a continuous random variable. 
We can always be more precise about the temperature by expanding to another decimal place (96 degrees, 96.44 degrees, 96.437 degrees, etc.).

Code:
mile_time = 'continuous'

candy = 'discrete'



Probability Mass Functions
A probability mass function (PMF) is a type of probability distribution that defines the probability of observing a particular value of a discrete random variable. 
For example, a PMF can be used to calculate the probability of rolling a three on a fair six-sided die.

There are certain kinds of random variables (and associated probability distributions) that are relevant for many different kinds of problems. 
These commonly used probability distributions have names and parameters that make them adaptable for different situations.

For example, suppose that we flip a fair coin some number of times and count the number of heads. 
The probability mass function that describes the likelihood of each possible outcome (eg., 0 heads, 1 head, 2 heads, etc.) is called the binomial distribution. The parameters for the binomial distribution are:

n for the number of trials (eg., n=10 if we flip a coin 10 times)
p for the probability of success in each trial (probability of observing a particular outcome in each trial. In this example, p= 0.5 because the probability of observing heads on a fair coin flip is 0.5)
If we flip a fair coin 10 times, we say that the number of observed heads follows a Binomial(n=10, p=0.5) distribution. The graph below shows the probability mass function for this experiment. 
The heights of the bars represent the probability of observing each possible outcome as calculated by the PMF.

Code:
See how the shape of the binomial distribution changes as the sample size changes.

The heights of the bars represent the probability of observing different values of heads from x number of fair coin flips. Taller bars represent more likely outcomes. For example, let’s say we flip a fair coin 5 times, and want to know the probability of getting between 1 and 3 heads. We can visualize this scenario with the probability mass function shown.

We can calculate this using the following equation where P(x) is the probability of observing the number x successes (heads in this case):

P(1to3heads)=P(1<=X<=3)
P(1to3heads)=P(X=1)+P(X=2)+P(X=3)
P(1to3heads)=0.1562+0.3125+0.3125
P(1to3heads)=0.7812
​
 
The sum of the heights of all the bars will always equal 1. So when x is larger, the number of heads we can observe increases, and the probability needs to be divided between more values.



Calculating Probabilities using Python
The binom.pmf() method from the scipy.stats library can be used to calculate the PMF of the binomial distribution at any value. This method takes 3 values:

x: the value of interest
n: the number of trials
p: the probability of success
For example, suppose we flip a fair coin 10 times and count the number of heads. 
We can use the binom.pmf() function to calculate the probability of observing 6 heads as follows:

# import necessary library
import scipy.stats as stats

# st.binom.pmf(x, n, p)
print(stats.binom.pmf(6, 10, 0.5))

Output:

# 0.205078

Notice that two of the three values that go into the stats.binomial.pmf() method are the parameters that define the binomial distribution: 
n represents the number of trials and p represents the probability of success.

Code:
