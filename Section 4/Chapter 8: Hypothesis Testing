cIntroduction to Hypothesis Testing (Simulating a One-Sample T-Test)
Learn about hypothesis testing by simulating a one-sample t-test

What is hypothesis testing?
Hypothesis Testing is a framework for asking questions about a dataset and answering them with probabilistic statements. 
There are many different kinds of hypothesis tests that can be used to address different kinds of questions and data. 
In this article, we’ll walk through a simulation of a one sample t- test in order to build intuition about how many different kinds of hypothesis tests work!

Step 1: Ask a Question
The International Baccalaureate Degree Programme (IBDP) is a world-wide educational program. 
Each year, students in participating schools can take a standardized assessment to earn their degree. 
Total scores on this assessment range from 1-45. Based on data provided here, the average total score for all students who took this exam in May 2020 was around 29.92. 
The distribution of scores looks something like this:

population_distribution

Imagine a hypothetical online school, Statistics Academy, that offers a 5-week test-preparation program. 
Suppose that 100 students who took the IBDP assessment in May 2020 were randomly 
chosen to participate in the first group of this program and that these 100 students earned an average score of 31.16 points on the exam — about 1.24 points higher than the international average! 
Are these students really outperforming their peers? Or could this difference be attributed to random chance?

Step 2: Define the Null and Alternative Hypotheses
Before attempting to answer this question, it is useful to reframe it in a way that is testable. 
Right now, our question (“Are the Statistics Academy students really outperforming their peers?”) is not clearly defined. 
Objectively, this group of 100 students performed better than the general population — but answering “yes, they outperformed their peers!” doesn’t feel particularly satisfying.

The reason it’s not satisfying is this: if we randomly choose ANY group of 100 students from the population of all test takers and calculate the average score for that sample, there’s a 50% chance it will be higher than the population average. Observing a higher average for a single sample is not surprising.

Of course, large differences from the population average are less probable — if all the Statistics Academy students earned 45 points on the exam (the highest possible score), 
we’d probably be convinced that these students had a real advantage. 
The trick is to quantify when differences are “large enough” to convince us that these students are systematically different from the general population. We can do this by reframing our question to focus on population(s), rather than our specific sample(s).

A hypothesis test begins with two competing hypotheses about the population that a particular sample comes from — in this case, the 100 Statistics Academy students:

Hypothesis 1 (technically called the Null Hypothesis): The 100 Statistics Academy students are a random sample from the general population of test takers, who had an average score of 29.92. 
If this hypothesis is true, the Statistics Academy students earned a slightly higher average score by random chance. Visually, this set-up looks something like this:
null hypothesis
Hypothesis 2 (technically called the Alternative Hypothesis): The 100 Statistics Academy students came from a population with an average score that is different from 29.92. 
In this hypothesis, we need to imagine two different populations that don’t actually exist: one where all students took the Statistics Academy test-prep program and one where none of the students took the program. 
If the alternative hypothesis is true, our sample of 100 Statistics Academy students came from a different population than the other test-takers. This can be visualized as follows:
alternative hypothesis
There’s one more clarification we need to make in order to fully specify the alternative hypothesis. 
Notice that, so far, we have not said anything about the average score for “population 1” in the diagram above, other than that it is NOT 29.92. 
We actually have three choices for what the alternative hypothesis says about this population average:

it is GREATER THAN 29.92
it is NOT EQUAL TO (i.e., GREATER THAN OR LESS THAN) 29.92
it is LESS THAN 29.92
Later on, we’ll discuss how the choice between “greater than”, “not equal to” and “less than” impacts the test.

Fill in the blank
Questions
Suppose that a random sample of 300 runners in the Boston Marathon were chosen to run in a new model of shoes: the Flying Flier. 
The average finishing time for these 300 runners was 230 minutes, while the average finishing time for all runners was 233 minutes. 
Was the average finishing time for this sample significantly different from the average finishing time for all runners?

Fill in the blanks to indicate the null and alternative hypotheses for the one-sample t-test to address this question.

Code
Null Hypothesis: The average finishing time for Boston Marathon runners wearing the Flying Flyer is blank 1 blank 2 minutes.

Alternative Hypothesis: The average finishing time for Boston Marathon runners wearing the Flying Flyer is blank 3 blank 4 minutes.
Answer Choices
233
not equal to
equal to
233
230
230
Click or drag and drop to fill in the blank

Step 3: Determine the Null Distribution
Now that we have our null hypothesis, we can generate a null distribution: the distribution (across repeated samples) of the statistic we are interested in if the null hypothesis is true. 
In the IBDP example described above, this is the distribution of average scores for repeated samples of size 100 drawn from a population with an average score of 29.92.

This might feel familiar if you’ve learned about the central limit theorem (CLT)! Statistical theory allows us to estimate the shape of this distribution using a single sample. 
You can learn how to do this by reading more about the CLT, but for the purposes of this example, let’s simulate the null distribution using our population. We can do this by:

Taking many random samples of 100 scores, each, from the population
Calculating and storing the average score for each of those samples
Plotting a histogram of the average scores
This yields the following distribution, which is approximately normal and centered at the population average:

sampling_distribution

If the null hypothesis is true, then the average score of 31.16 observed among Statistics Academy students is simply one of the values from this distribution. 
Let’s plot the sample average on the null distribution. Notice that this value is within the range of the null distribution, but it is off to the right where there is less density:

population_distribution_with_sample_mean

Step 4: Calculate a P-Value (or Confidence Interval)
Here is the basic question asked in our hypothesis test:

Given that the null hypothesis is true (that the 100 students from Statistics Academy were sampled from a population with an average IBDP score of 29.92), how likely is it that their average score is 31.16?

The minor problem with this question is that the probability of any exact average score is very small, so we really want to estimate the probability of a range of scores. Let’s now return to our three possible alternative hypotheses and see how the question and calculation each change slightly, depending on which one we choose:

Option 1
Alternative Hypothesis: The sample of 100 scores earned by Statistics Academy students came from a population with an average score that is greater than 29.92.

In this case, we want to know the probability of observing a sample average greater than or equal to 31.16 given that the null hypothesis is true. 
Visually, this is the proportion of the null distribution that is greater than or equal to 31.16 (shaded in red below). Here, the red region makes up about 3.1% of the total distribution. 
This proportion, which is usually written in decimal form (i.e., 0.031), is called a p-value.

one_sided_upper_test

Option 2
Alternative Hypothesis: The sample of 100 scores earned by Statistics Academy students came from a population with an average score that is not equal to (i.e., greater than OR less than) 29.92.

We observed a sample average of 31.16 among the Statistics Academy students, which is 1.24 points higher than the population average (if the null hypothesis is true) of 29.92. 
In the first version of the hypothesis test (option 1), we estimated the probability of observing a sample average that is at least 1.24 points higher than the population average. 
For the alternative hypothesis described in option 2, we’re interested in the probability of observing a sample average that is at least 1.24 points DIFFERENT (higher OR lower) than the population average. 
Visually, this is the proportion of the null distribution that is at least 1.24 units away from the population average (shaded in red below). 
Note that this area is twice as large as in the previous example, leading to a p-value that is also twice as large: 0.062.

two_sided_test

While option 1 is often referred to as a One Sided or One-Tailed test, this version is referred to as a Two Sided or Two-Tailed test, referencing the two tails of the null distribution that are counted in the p-value. 
It is important to note that most hypothesis testing functions in Python and R implement a two-tailed test by default.

Option 3
Alternative Hypothesis: The sample of 100 scores earned by Statistics Academy students came from a population with an average score that is less than 29.92.

Here, we want to know the probability of observing a sample average less than or equal to 31.16, given that the null hypothesis is true. This is also a one-sided test, just the opposite side from option 1. 
Visually, this is the proportion of the null distribution that is less than or equal to 31.16. This is a very large area (almost 97% of the distribution!), leading to a p-value of 0.969.

lower_tail_test

At this point, you may be thinking: why would anyone ever choose this version of the alternative hypothesis?! In real life, if a test-prep program was planning to run a rigorous experiment to see whether their students are scoring differently than the general population, they should choose the alternative hypothesis before collecting any data. At that point, they won’t know whether their sample of students will have an average score that is higher or lower than the population average — although they probably hope that it is higher.

Step 5: Interpret the Results
In the three examples above, we calculated three different p-values (0.031, 0.062, and 0.969, respectively). Consider the first p-value of 0.031. The interpretation of this number is as follows:

If the 100 students at Statistics Academy were randomly selected from the full population (which had an average score of 29.92), there is a 3.1% chance of their average score being 31.16 points or higher.

This means that it is relatively unlikely, but not impossible, that the Statistics Academy students scored higher (on average) than their peers by random chance, despite no real difference at the population level. In other words, the observed data is unlikely if the null hypothesis is true. Note that we have directly tested the null hypothesis, but not the alternative hypothesis! We therefore need to be a little careful about how we interpret this test: we cannot say that we’ve proved that the alternative hypothesis is the truth — only that the data we collected would be unlikely under null hypothesis, and therefore we believe that the alternative hypothesis is more consistent with our observations.

Fill in the blank
Questions
Suppose that a random sample of 300 runners in the Boston Marathon were chosen to run in a new model of shoes: the Flying Flier. 
The average finishing time for these 300 runners was 230 minutes, while the average finishing time for all runners was 233 minutes. 
We ran a 2-sided one-sample t-test with the following null and alternative hypotheses:

Null Hypothesis: The average finishing time for Boston Marathon runners wearing the Flying Flyer is equal to 233 minutes.

Alternative Hypothesis: The average finishing time for Boston Marathon runners wearing the Flying Flyer is not equal to 233 minutes

If we get a p-value of 0.10 for this test, fill in the blanks to provide a correct interpretation of that number.

Code
If the 300 runners were randomly selected from the full population of runners (who had an average finishing time of 233 minutes), 
there is a blank 1 chance of their average finishing time being at least 3 minutes blank 2 the average finishing time of all runners.
Answer Choices
different from
faster than
10%
slower than
.10%
Click or drag and drop to fill in the blank

Significance thresholds
While it is perfectly reasonable to report a p-value, many data scientists use a predetermined threshold to decide whether a particular p-value is significant or not.
P-values below the chosen threshold are declared significant and lead the data scientist to “reject the null hypothesis in favor of the alternative”. 
A common choice for this threshold, which is also sometimes referred to as Alpha, is 0.05 — but this is an arbitrary choice! 
Using a lower threshold means you are less likely to find significant results, but also less likely to mistakenly report a significant result when there is none.

Using the first p-value of 0.031 and a significance threshold of 0.05, 
Statistics Academy could reject the null hypothesis and conclude that the 100 students who participated in their program scored significantly higher on the test than the general population.

Impact of the Alternative Hypothesis
Note that different alternative hypotheses can lead to different conclusions. 
For example, the one-sided test described above (p = 0.031) would lead a data scientist to reject the null at a 0.05 significance level. 
Meanwhile, a two-sided test on the same data leads to a p-value of 0.062, which is greater than the 0.05 threshold. 
Thus, for the two-sided test, a data scientist could not reject the null hypothesis. This highlights the importance of choosing an alternative hypothesis ahead of time!

Summary and further applications
As you have hopefully learned from this article, hypothesis testing can be a useful framework for asking and answering questions about data. 
Before you use hypothesis tests in practice, it is important to remember the following:

A p-value is a probability, usually reported as a decimal between zero and one.
A small p-value means that an observed sample statistic (or something more extreme) would be unlikely to occur if the null hypothesis is true.
A significance threshold can be used to translate a p-value into a “significant” or “non-significant” result.
In practice, the alternative hypothesis and significance threshold should be chosen prior to data collection.
There are many different hypothesis tests that can be used to answer different kinds of questions about data. 
Now that you’ve learned about one, you have all the skills you’ll need to understand many more!



One-Sample T-Tests in SciPy
Introduction
In this lesson, we’ll walk through the implementation of a one-sample t-test in Python. One-sample t-tests are used for comparing a sample average to a hypothetical population average. 
For example, a one-sample t-test might be used to address questions such as:

Is the average amount of time that visitors spend on a website different from 5 minutes?
Is the average amount of money that customers spend on a purchase more than 10 USD?
As an example, let’s imagine the fictional business BuyPie, which sends ingredients for pies to your household so that you can make them from scratch. 
Suppose that a product manager wants online BuyPie orders to cost around 1000 Rupees on average. 
In the past day, 50 people made an online purchase and the average payment per order was less than 1000 Rupees. 
Are people really spending less than 1000 Rupees on average? Or is this the result of chance and a small sample size?

Code:
import numpy as np

prices = np.genfromtxt("prices.csv")

# print prices:
print(prices)

# calculate prices_mean and print it out:
prices_mean = np.mean(prices)
print(prices_mean)



Implementing a One-Sample T-Test
In the last exercise, we inspected a sample of 50 purchase prices at BuyPie and saw that the average was 980 Rupees. 
Suppose that we want to run a one-sample t-test with the following null and alternative hypotheses:

Null: The average cost of a BuyPie order is 1000 Rupees
Alternative: The average cost of a BuyPie order is not 1000 Rupees.
SciPy has a function called ttest_1samp(), which performs a one-sample t-test for you. ttest_1samp() requires two inputs, a sample distribution (eg. the list of the 50 observed purchase prices) 
and a mean to test against (eg. 1000):

tstat, pval = ttest_1samp(sample_distribution, expected_mean)

The function uses your sample distribution to determine the sample size and estimate the amount of variation in the population — which are used to estimate the null distribution. 
It returns two outputs: the t-statistic (which we won’t cover in this course), and the p-value.



from scipy.stats import ttest_1samp
import numpy as np

prices = np.genfromtxt("prices.csv")
print(prices)

prices_mean = np.mean(prices)
print("mean of prices: " + str(prices_mean))

# use ttest_1samp to calculate pval
tstat, pval = ttest_1samp(prices, 1000)
# print pval
print(pval)



Assumptions of a One Sample T-Test
When running any hypothesis test, it is important to know and verify the assumptions of the test. The assumptions of a one-sample t-test are as follows:

The sample was randomly selected from the population
For example, if you only collect data for site visitors who agree to share their personal information, this subset of visitors was not randomly selected and may differ from the larger population.

The individual observations were independent
For example, if one visitor to BuyPie loves the apple pie they bought so much that they convinced their friend to buy one too, those observations were not independent.

The data is normally distributed without outliers OR the sample size is large (enough)
There are no set rules on what a “large enough” sample size is, but a common threshold is around 40. 
For sample sizes smaller than 40, and really all samples in general, it’s a good idea to make sure to plot a histogram of your data and check for outliers, 
multi-modal distributions (with multiple humps), or skewed distributions. If you see any of those things for a small sample, a t-test is probably not appropriate.

In general, if you run an experiment that violates (or possibly violates) one of these assumptions, you can still run the test and report the results — 
but you should also report assumptions that were not met and acknowledge that the test results could be flawed.

Code:
import codecademylib3
import numpy as np
import matplotlib.pyplot as plt

prices = np.genfromtxt("prices.csv")

#plot your histogram here
plt.hist(prices)
plt.show()



Review
Congratulations! You now know how to implement a one-sample t-test in Python and verify the assumptions of the test. To recap, here are some of the things you learned:

One-sample t-tests are used for comparing a sample mean to an expected population mean
A one-sample t-test can be implemented in Python using the SciPy ttest_1samp() function
Assumptions of a one-sample t-test include:
The sample was randomly drawn from the population of interest
The observations in the sample are independent
The sample size is large “enough” or the sample data is normally distributed
Instructions
As a final exercise, some data has been loaded for you with purchase prices for consecutive days at BuyPie.
You can access the first day using daily_prices[0], the second using daily_prices[1], etc.. 
To practice running a one-sample t-test and inspecting the resulting p-value, try the following:

Calculate and print out a p-value for day 1 where the null hypothesis is that the average purchase price was 1000 Rupees and the alternative hypothesis is that the average purchase price was not 1000 Rupees. 
Print out the p-value.

Run the same hypothesis tests for days 1-10 (the fastest way to do this is with a for-loop!) and print out the resulting p-values. 
What’s the smallest p-value you observe for those 10 days?

Try changing the null hypothesis so that the expected population mean that you’re testing against is different from 1000. 
Try any numbers that you want. How do your p-values change?

Solution code can be found in solution.py

Code:
from scipy.stats import ttest_1samp, shapiro
import numpy as np
import matplotlib.pyplot as plt

# Load daily prices data
daily_prices = np.genfromtxt("daily_prices.csv", delimiter=",")

# Print the prices for Day 1 to verify the data
print('Prices for Day 1:', daily_prices[0])

# Create a histogram to visualize values in the dataset for Day 1
plt.hist(daily_prices[0], edgecolor='black', bins=20)
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.title('Histogram of Daily Prices for Day 1')
plt.show()

# Perform Shapiro-Wilk test for normality on each dataset
for x in range(10):
    stat, p_value = shapiro(daily_prices[x])
    print(f'Day {x + 1}: Shapiro-Wilk Test Statistic = {stat}, p-value = {p_value}')

    # Interpret the Shapiro-Wilk test results for each dataset
    if p_value < 0.05:
        print(f"Day {x + 1}: The data does not follow a normal distribution.\n")
    else:
        print(f"Day {x + 1}: The data appears to follow a normal distribution.\n")

# Task 1 - Calculate and print p-value for day 1
# H0: average price was 1000 rupees and 
# Ha: average price != 1000 rupees.

# Counting the sample size for day 1
print('Sample Size for Day 1:', np.count_nonzero(daily_prices[0]))

# Calculate the mean for day 1
daily_prices_mean_day_1 = np.mean(daily_prices[0])
print('Mean for Day 1:', daily_prices_mean_day_1)

# Perform the one-sample t-test
tstat, p_value = ttest_1samp(daily_prices[0], 1000)

# Print the p-value
print('P-value:', p_value)

# Interpret the p-value result
if p_value < 0.05:
    print("The average daily price is significantly different from the population mean of 1000 Rupees.")
else:
    print("There is no significant difference in the average daily price compared to 1000 Rupees.")

# Task 2: Run the same hypothesis tests for days 1-10 and print out the resulting p-values.
p_values = []
for x in range(10):
    tstat, p_value = ttest_1samp(daily_prices[x], 1000)
    p_values.append(p_value)
    print(f"Day {x + 1}: p-value = {p_value}")

# Find the smallest p-value among the 10 days
min_p_value = min(p_values)
print(f"The smallest p-value observed for days 1-10 is: {min_p_value}")

# Task 3: Change the null hypothesis so that the expected population mean is different from 1000.
print("Day 1-10 with a different null hypothesis (H0 = 1100):")
for x in range(10):
    tstat, pval = ttest_1samp(daily_prices[x], 1100)
    print(f"Day {x + 1}: p-value = {pval}")  # Use pval instead of p_value




--Simulating a Binomial Test--

Introduction
In this lesson, we will walk through a simulation of a binomial hypothesis test in Python. 
Binomial tests are useful for comparing the frequency of some outcome in a sample to the expected probability of that outcome. 
For example, if we expect 90% of ticketed passengers to show up for their flight but only 80 of 100 ticketed passengers actually show up, 
we could use a binomial test to understand whether 80 is significantly different from 90.

Binomial tests are similar to one-sample t-tests in that they test a sample statistic against some population-level expectation. The difference is that:

binomial tests are used for binary categorical data to compare a sample frequency to an expected population-level probability
one-sample t-tests are used for quantitative data to compare a sample mean to an expected population mean.
In Python, as in many other programming languages used for statistical computing, 
there are a number of libraries and functions that allow a data scientist to run a hypothesis test in a single line of code. 
However, a data scientist will be much more likely to spot and fix potential errors and interpret results correctly 
if they have a conceptual understanding of how these functions work. To that end, this lesson will help you build your own conceptual understanding!

Code:
import pandas as pd
import codecademylib3

monthly_report = pd.read_csv('monthly_report.csv')

print(monthly_report.head())



Summarizing the Sample
The marketing department at Live-it-LIVE reports that, during this time of year, about 10% of visitors to Live-it-LIVE.com make a purchase.

The monthly report shows every visitor to the site and whether or not they made a purchase. The checkout page had a small bug this month, 
so the business department wants to know whether the purchase rate dipped below expectation. They’ve asked us to investigate this question.

In order to run a hypothesis test to address this, we’ll first need to know two things from the data:

The number of people who visited the website
The number of people who made a purchase on the website
Assuming each row of our dataset represents a unique site visitor, we can calculate the number of people who visited the website by finding the number of rows in the data frame. 
We can then find the number who made a purchase by using a conditional statement to add up the total number of rows where a purchase was made.

For example, suppose that the dataset candy contains a column named chocolate with 'yes' recorded for every candy that has chocolate in it and 'no' otherwise. 
The following code calculates the sample size (the number of candies) and the number of those candies that contain chocolate:

## sample size (number of rows): 
samp_size = len(candy)
 
## number with chocolate: 
total_with_chocolate = np.sum(candy.chocolate == 'yes')

Code:
import numpy as np
import pandas as pd
import codecademylib3

monthly_report = pd.read_csv('monthly_report.csv')

#print the head of monthly_report:
print(monthly_report.head())

#calculate and print sample_size:
sample_size = len(monthly_report)
print('sample size:')
print(sample_size)

#calculate and print num_purchased:
num_purchased = np.sum(monthly_report.purchase == 'y')
print("number of purchases:")
print(num_purchased)



Simulating Randomness
In the last exercise, we calculated that there were 500 site visitors to live-it-LIVE.com this month and 41 of them made a purchase. 
In comparison, if each of the 500 visitors had a 10% chance of making a purchase, we would expect around 50 of those visitors to buy something. 
Is 41 different enough from 50 that we should question whether this months’ site visitors really had a 10% chance of making a purchase?

To conceptualize why our expectation (50) and observation (41) might not be equal — EVEN IF there was no dip in the purchase probability — let’s turn to a common probability example: flipping a fair coin. 
We can simulate a coin flip in Python using the numpy.random.choice() function:

flip = np.random.choice(['heads', 'tails'], size=1, p=[0.5, 0.5])
print(flip) 
## output is either ['heads'] or ['tails']

If we run this code (or flip a real coin) a few times, we’ll find that — just like we can’t know ahead of time whether any single visitor to Live-it-LIVE.com 
will make a purchase — we can’t predict the outcome of any individual coin flip.

If we flip a fair coin 10 times in a row, we expect about 5 of those coins to come up heads (50%). We can simulate this in python by changing the size parameter of numpy.random.choice():

flip = np.random.choice(['heads', 'tails'], size=10, p=[0.5, 0.5])
print(flip)
## output is something like: ['heads' 'heads' 'heads' 'tails' 'tails' 'heads' 'heads' 'tails' 'heads' 'heads']

If you try this yourself, it’s perfectly reasonable that you’ll get only four heads, or maybe six or seven! Because this is a random process, 
we can’t guarantee that exactly half of our coin flips will come up heads. Similarly, even if each Live-it-LIVE visitor has a 10% chance of making a purchase, 
that doesn’t mean we expect exactly 10% to do so in any given sample.

Code:
import numpy as np
import pandas as pd

monthly_report = pd.read_csv('monthly_report.csv')

#simulate one visitor:
one_visitor = np.random.choice(['y','n'], size=1, p=[0.1,0.9])
print(one_visitor)

#simulate 500 visitors:
simulated_monthly_visitors = np.random.choice(['y','n'], size=500, p=[0.1,0.9])
print(simulated_monthly_visitors)



Simulating the Null Distribution I
The first step in running a hypothesis test is to form a null hypothesis. For the question of whether the purchase rate at Live-it-LIVE.com was different from 10% this month, 
the null hypothesis describes a world in which the true probability of a visitor making a purchase was exactly 10%, but by random chance, we observed that only 41 visitors (8.2%) made a purchase.

Let’s return to the coin flip example from the previous exercise. We can simulate 10 coin flips and print out the number of those flips that came up heads using the following code:

flips = np.random.choice(['heads', 'tails'], size=10, p=[0.5, 0.5])
num_heads = np.sum(flips == 'heads')
print(num_heads)
## output: 4

If we run this code a few times, we’ll likely see different results each time. This will give us get a sense for the range in the number of heads that could occur by random chance, 
even if the coin is fair. We’re more likely to see numbers like four, five, or six, but maybe we’ll see something more extreme every once in a while — ten heads in a row, or even zero!

Code:
import numpy as np
import pandas as pd

monthly_report = pd.read_csv('monthly_report.csv')

#simulate 500 visitors:
simulated_monthly_visitors = np.random.choice(['y', 'n'], size=500, p=[0.1, 0.9])

#calculate the number of simulated visitors who made a purchase:
num_purchased = np.sum(simulated_monthly_visitors == 'y')

print(num_purchased)

#Initial run: 53, After 5 runs: 45, 47, 55, 44, 54. Furthest number from 50 is 55 



Simulating the Null Distribution II
In the last exercise, we simulated a random sample of 500 visitors, where each visitor had a 10% chance of making a purchase. 
When we pressed “Run” a few times, we saw that the number of purchases varied from sample to sample, but was around 50.

Similarly, we simulated a single random sample of 10 coin flips, where each flip had a 50% chance of coming up heads. 
We saw that the number of simulated heads was not necessarily 5, but somewhere around 5.

By running the same simulated experiment many times, we can get a sense for how much a particular outcome (like the number of purchases, or heads) varies by random chance. 
Consider the following code:

outcomes = []
for i in range(10000): 
    flips = np.random.choice(['heads', 'tails'], size=10, p=[0.5, 0.5])
    num_heads = np.sum(flips == 'heads')
    outcomes.append(num_heads)
print(outcomes)
## output is something like: [3, 4, 5, 8, 5, 6, 4, 5, 3, 2, 8, 5, 7, 4, 4, 5, 4, 3, 6, 5,...]

In this code chunk, we’ve done the following:

initialized an empty list named outcomes to store the number of ‘heads’ from simulated samples of coin flips
set up a for-loop to repeat the steps below 10000 times:
flip a fair coin 10 times
calculate the number of those 10 flips that came up heads
append that number onto outcomes
Note that 10000 is an arbitrarily chosen large number — it’s big enough that it will yield almost all possible outcomes of our experiment, 
and small enough that the simulation still runs quickly. 

From inspecting the output, we can see that the number of ‘heads’ varied between 0 and 10:

min_heads = np.min(outcomes) 
print(min_heads) #output: 0
 
max_heads = np.max(outcomes)
print(max_heads) #output: 10

Thus, if we flip a fair coin 10 times, we could observe anywhere between 0 and 10 heads by random chance.

Code:
import numpy as np
import pandas as pd

null_outcomes = []

#start for loop here:
for i in range(10000):
  
  simulated_monthly_visitors = np.random.choice(['y', 'n'], size=500, p=[0.1, 0.9])

  num_purchased = np.sum(simulated_monthly_visitors == 'y')

  null_outcomes.append(num_purchased) 


#calculate the minimum and maximum values in null_outcomes here:
null_min = np.min(null_outcomes)
print('Min value for null outcomes', null_min)

null_max = np.max(null_outcomes)
print('Max value for null outcomes',null_max)



Inspecting the Null Distribution
In the previous exercise, we simulated 10000 different samples of 500 visitors, where each visitor had a 10% chance of making a purchase, and calculated the number of purchases per sample. 
Upon further inspection, we saw that those numbers ranged from around 25 to 75. This is useful information, but we can learn even more from inspecting the full distribution.

For example, recall our 10000 coin flip experiments: for each experiment, we flipped a fair coin 10 times and recorded the number of heads in a list named outcomes. 
We can plot a histogram of outcomes using matplotlib.pyplot.hist(). We can also add a vertical line at any x-value using matplotlib.pyplot.axvline():

import matplotlib.pyplot as plt
plt.hist(outcomes)
plt.axvline(2, color = 'r')
plt.show()

Output:

distribution of coin flips

This histogram shows us that, over 10000 experiments, we observed as few as 0 and as many as 10 heads out of 10 flips. 
However, we were most likely to observe around 4-6 heads. It would be unlikely to observe only 2 heads (where the vertical red line is).

Code:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import codecademylib3

null_outcomes = []

for i in range(10000):
  simulated_monthly_visitors = np.random.choice(['y', 'n'], size=500, p=[0.1, 0.9])

  num_purchased = np.sum(simulated_monthly_visitors == 'y')

  null_outcomes.append(num_purchased)

#plot the histogram here:
plt.hist(null_outcomes)
plt.axvline(41, color = 'r')
plt.show()
plt.close()



Confidence Intervals
So far, we’ve inspected the null distribution and calculated the minimum and maximum values. 
While the number of purchases in each simulated sample ranged roughly from 25 to 75 by random chance, upon further inspection of the distribution, we saw that those extreme values happened very rarely.

By reporting an interval covering 95% of the values instead of the full range, we can say something like: “we are 95% confident that, if each visitor has a 10% chance of making a purchase, 
a random sample of 500 visitors will make between 37 and 63 purchases.” We can use the np.percentile() function to calculate this 95% interval as follows:

np.percentile(outcomes, [2.5,97.5])
# output: [37. 63.]

We calculated the 2.5th and 97.5th percentiles so that exactly 5% of the data falls outside those percentiles (2.5% above the 97.5th percentile, and 2.5% below the 2.5th percentile). 
This leaves us with a range covering 95% of the data.

If our observed statistic falls outside this interval, then we can conclude it is unlikely that the null hypothesis is true. 
In this example, because 41 falls within the 95% interval (37 - 63), it is still reasonably likely that we observed a lower purchase rate by random chance, even though the null hypothesis was true.

Code:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import codecademylib3

null_outcomes = []

for i in range(10000):
  simulated_monthly_visitors = np.random.choice(['y', 'n'], size=500, p=[0.1, 0.9])

  num_purchased = np.sum(simulated_monthly_visitors == 'y')

  null_outcomes.append(num_purchased)

#calculate the 90% interval here:
null_90CI = np.percentile(null_outcomes, [5,95])
print(null_90CI)



Calculating a One-Sided P-Value
P-value calculations and interpretations depend on the alternative hypothesis of a test, a description of the difference from expectation that we are interested in.

For example, let’s return to the 10-coin-flip example from earlier. Suppose that we flipped a coin 10 times and observed only 2 heads. 
We might run a hypothesis test with the following null and alternative hypotheses:

Null: the probability of heads is 0.5
Alternative: the probability of heads is less than 0.5
This hypothesis test asks the question: IF the probability of heads is 0.5, what’s the probability of observing 2 or fewer heads among a single sample of 10 coin flips?

Earlier, we used a for-loop to repeatedly (10000 times!) flip a fair coin 10 times, and store the number of heads (for each set of 10 flips) in a list named outcomes. 
The probability of observing 2 or fewer heads among 10 coin flips is approximately equal to the proportion of those 10000 experiments where we observed 0, 1, or 2 heads:

import numpy as np
outcomes = np.array(outcomes)
p_value = np.sum(outcomes <= 2)/len(outcomes) 
print(p_value) #output: 0.059

This calculation is equivalent to calculating the proportion of this histogram that is colored in red:null distribution with bars colored red for values less than or equal to 2

We estimated that the probability of observing 2 or fewer heads is about 0.059 (5.9%). This probability (0.059) is referred to as a one-sided p-value.

Code:
import numpy as np
import pandas as pd

null_outcomes = []

for i in range(10000):
  simulated_monthly_visitors = np.random.choice(['y', 'n'], size=500, p=[0.1, 0.9])

  num_purchased = np.sum(simulated_monthly_visitors == 'y')

  null_outcomes.append(num_purchased)

#calculate the p-value here:
null_outcomes = np.array(null_outcomes)
p_value = np.sum(null_outcomes <= 41)/len(null_outcomes)
print(p_value)



Calculating a Two-Sided P-Value
In the previous exercise, we calculated a one-sided p-value. In this exercise, we’ll estimate a p-value for a 2-sided test, which is the default setting for many functions in Python (and other languages, like R!).

In our 10-coin-flip experiment, remember that we observed 2 heads, which is 3 less than the expected value of 5 (50% of 10) if the null hypothesis is true. 
The two sided test focuses on the number of heads being three different from expectation, rather than just less than. The hypothesis test now asks the following question:

Suppose that the true probability of heads is 50%. What is the probability of observing either two or fewer heads OR eight or more heads? (Note that two and eight are both three away from five). 
The calculation now estimates the proportion of the null histogram that is colored in red:

null distribution for 10 coin flips with a probability of heads equal to 0.5, and all bars above x-values <=2 or >=8 are shaded red, illustrating a two-sided hypothesis test

This proportion can be calculated in Python as follows. Note that the | symbol is similar to 'or', but works for comparing multiple values at once.

import numpy as np
outcomes = np.array(outcomes)
p_value = np.sum((outcomes <= 2) | (outcomes >= 8))/len(outcomes)
print(p_value) #output: 0.12

We end up with a p-value that is twice as large as the one-sided p-value.

Code:
import numpy as np
import pandas as pd

null_outcomes = []

for i in range(10000):
  simulated_monthly_visitors = np.random.choice(['y', 'n'], size=500, p=[0.1, 0.9])

  num_purchased = np.sum(simulated_monthly_visitors == 'y')

  null_outcomes.append(num_purchased)

#calculate the p-value here:
null_outcomes = np.array(null_outcomes)
p_value = np.sum((null_outcomes <= 41) | (null_outcomes >= 59)) / len(null_outcomes)
print(p_value)



Writing a Binomial Test Function
So far, we’ve conducted a simulated binomial hypothesis test for Live-it-LIVE.com. In this exercise, we’ll use our code from the previous exercises to write our own binomial test function. 
Our function will use simulation, so it will estimate (albeit fairly accurately) the same p-values we would get using much more complex mathematical equations.

A function has been outlined for you in script.py which contains the code that we used for Live_it_LIVE inside a function named simulation_binomial_test(). 
Your goal in the next few exercises will be to edit this function so that it takes in any values for the following:

The observed sample statistic (eg., 41 purchases)
The sample size (eg., 500 visitors)
The null probability of success (eg., 0.10 probability of a purchase)
The function should return a p-value for a one-sided test where the alternative hypothesis is that the true probability of success is LESS THAN the null.

Code:
import numpy as np
import pandas as pd
from scipy.stats import binom_test

def simulation_binomial_test(observed_successes, n, p):
  #initialize null_outcomes
  null_outcomes = []
  
  #generate the simulated null distribution
  for i in range(10000):
    simulated_monthly_visitors = np.random.choice(['y', 'n'], size=n, p=[p, 1-p])
    num_purchased = np.sum(simulated_monthly_visitors == 'y')
    null_outcomes.append(num_purchased)

  #calculate a 1-sided p-value
  null_outcomes = np.array(null_outcomes)
  p_value = np.sum(null_outcomes <= observed_successes)/len(null_outcomes) 
  
  #return the p-value
  return p_value

#Test your function below by uncommenting the code below. You should see that your simulation function gives you a very similar answer to the binom_test function from scipy:

p_value1 = simulation_binomial_test(45, 500, .1)
print("simulation p-value: ", p_value1)

p_value2 = binom_test(45, 500, .1, alternative = 'less')
print("binom_test p-value: ", p_value2)



Binomial Testing with SciPy
Congratulations — if you’ve made it this far, you’ve now written your own function to simulate a binomial test!

More formally, the binomial distribution describes the number of expected “successes” in an experiment with some number of “trials”. 
In the example you just worked through, the experiment consisted of 500 people visiting Live-it-LIVE.com. For each of those trials (visitors), we expected a 10% chance of a purchase (success), 
but observed only 41 successes (less than 10%).

SciPy has a function called binom_test(), which performs a binomial test for you. The default alternative hypothesis for the binom_test() function is two-sided, 
but this can be changed using the alternative parameter (eg., alternative = 'less' will run a one-sided lower tail test).

binom_test() requires three inputs, the number of observed successes, the number of total trials, and an expected probability of success. 
For example, with 10 flips of a fair coin (trials), the expected probability of heads is 0.5. Let’s imagine we get 2 heads (observed successes) in 10 flips. 
Is the coin weighted? The function call for this binomial test would look like:

from scipy import binom_test
p_value = binom_test(2, n=10, p=0.5)
print(p_value) #output: 0.109

This tells us that IF the true probability of heads is 0.5, the probability of observing 2 or fewer heads OR 8 or more heads is 0.109 (10.9%).

Code:
import numpy as np
import pandas as pd
from scipy.stats import binom_test

# calculate p_value_2sided here:
p_value_2sided = binom_test(41, n=500, p=0.1)
print(p_value_2sided)

# calculate p_value_1sided here:
p_value_1sided = binom_test(41, n=500, p=0.1, alternative= 'less')
print(p_value_1sided)



Review
Congratulations! You now know how to run a binomial hypothesis test using a SciPy function — or by simulating it yourself! 
This will serve you well as a data scientist because it will enable you to investigate what’s going on if pre-written functions return surprising results. 
You also now have a conceptual understanding of how a binomial test works and what questions it aims to answer. 

To summarize, here are some of the things you’ve learned about hypothesis tests in general:

All hypothesis tests start with a null and alternative hypothesis

Outcomes of a hypothesis test that might be reported include:

confidence intervals
p-values
A hypothesis test can be simulated by:

taking repeated random samples where the null hypothesis is assumed to be true
using those simulated samples to generate a null distribution
comparing an observed sample statistic to that null distribution

Instructions
As a final exercise, the solution code for the previous exercise is available to you in script.py. 

As a challenge, see if you can re-write the simulation-based binomial test function so that it has an input named alternative_hypothesis that can be equal to 'less', 'not_equal', or 'greater'. 

Then change the function so that it performs the appropriate one- or two-sided test for the alternative hypothesis provided. 
Solution code is available to you in solution.py if interested.

Code:
import numpy as np
from scipy.stats import binom_test

def simulation_binomial_test(observed_successes, n, p, alternative_hypothesis):
    # Validate the input for the alternative hypothesis
    if alternative_hypothesis not in ['less', 'greater', 'not_equal']:
        raise ValueError("Invalid input! Choose 'less', 'greater', or 'not_equal'.")

    # Initialize null_outcomes
    null_outcomes = []

    # Generate the simulated null distribution
    for i in range(10000):
        simulated_monthly_visitors = np.random.choice(['y', 'n'], size=n, p=[p, 1-p])
        num_purchased = np.sum(simulated_monthly_visitors == 'y')
        null_outcomes.append(num_purchased)
  
    # Convert outcomes to a numpy array
    null_outcomes = np.array(null_outcomes)

    # Calculate p-value based on the alternative hypothesis
    if alternative_hypothesis == 'less':
        p_value = np.sum(null_outcomes <= observed_successes) / len(null_outcomes)
    elif alternative_hypothesis == 'greater':
        p_value = np.sum(null_outcomes >= observed_successes) / len(null_outcomes)
    elif alternative_hypothesis == 'not_equal':
        p_value = np.sum(
          np.abs(null_outcomes - np.mean(null_outcomes)) >= np.abs(observed_successes - np.mean(null_outcomes))) / len(null_outcomes)

    # Return the p-value
    return p_value

# Example usage (without input prompt):
observed_successes = 41  # Number of observed purchases
n = 500                  # Number of visitors
p = 0.10                 # Hypothesized purchase rate (10%)
alternative_hypothesis = 'less'  # Change this to 'less', 'greater', or 'not_equal'

p_value = simulation_binomial_test(observed_successes, n, p, alternative_hypothesis)
print(f"Calculated p-value: {p_value}")



Significance Thresholds
Introduction to Significance Thresholds
Sometimes, when we run a hypothesis test, we simply report a p-value or a confidence interval and give an interpretation 
(eg., the p-value was 0.05, which means that there is a 5% chance of observing two or fewer heads in 10 coin flips).

In other situations, we want to use our p-value to make a decision or answer a yes/no question. 
For example, suppose that we’re developing a new quiz question at Codecademy and want learners to have a 70% chance of getting the question right 
(higher would mean the question is too easy, lower would mean the question is too hard). 
We show our quiz question to a sample of 100 learners and 60 of them get it right. 
Is this significantly different from our target of 70%? If so, we want to remove the question and try to rewrite it.

In order to turn a p-value, which is a probability, into a yes or no answer, data scientists often use a pre-set significance threshold. 
The significance threshold can be any number between 0 and 1, but a common choice is 0.05. 
P-values that are less than this threshold are considered “significant”, while larger p-values are considered “not significant”.

Code:
# P-Value for first Hypothesis Test
p_value1 = .062
# Set the correct value for p_value1_significance
p_value1_significance = 'not significant'

# P-Value for second Hypothesis Test
p_value2 = 0.013
# Set the correct value for p_value2_significance
p_value2_significance = 'significant'



Interpreting a P-Value based on a Significance Threshold
Let’s return to the quiz question example from the previous exercise — 
we want to remove our quiz question from our website if the probability of a correct response is different from 70%. 
Suppose we collected data from 100 learners and ran a binomial hypothesis test with the following null and alternative hypotheses:

Null: The probability that a learner gets the question correct is 70%.
Alternative: The probability that a learner gets the question correct is not 70%.
Assuming that we set a significance threshold of 0.05 for this test:

If the p-value is less than 0.05, the p-value is significant. We will “reject the null hypothesis” and conclude that the probability of a correct answer is significantly different from 70%. 
This would lead us to re-rewrite the question.
If the p-value is greater than 0.05, the p-value is not significant. 
We will not be able to reject the null hypothesis, and will conclude that the probability of a correct answer is not significantly different from 70%. 
This would lead us to leave the question on the site.

Code:
# P-Value for first Hypothesis Test
p_value1 = .062
# Set the correct value for remove_question_1
remove_question_1 = 'no'

# P-Value for second Hypothesis Test
p_value2 = 0.013
# Set the correct value for remove_question_2
remove_question_2 = 'yes'



Error Types
Whenever we run a hypothesis test using a significance threshold, we expose ourselves to making two different kinds of mistakes: 
type I errors (false positives) and type II errors (false negatives):

Null hypothesis:	is true	is false
P-value significant	Type I Error	Correct!
P-value not significant	Correct!	Type II error

Consider the quiz question hypothesis test described in the previous exercises:

Null: The probability that a learner answers a question correctly is 70%.
Alternative: The probability that a learner answers a question correctly is not 70%.
Suppose, for a moment, that the true probability of a learner answering the question correctly is 70% (if we showed the question to ALL learners, exactly 70% would answer it correctly). 
This puts us in the first column of the table above (the null hypothesis “is true”). If we run a test and calculate a significant p-value, 
we will make type I error (also called a false positive because the p-value is falsely significant), leading us to remove the question when we don’t need to.

On the other hand, if the true probability of getting the question correct is not 70%, the null hypothesis “is false” (the right-most column of our table). 
If we run a test and calculate a non-significant p-value, we make a type II error, leading us to leave the question on our site when we should have taken it down.

Code:
# Set the correct value for outcome
outcome = 'type two'



Setting the Type I Error Rate
It turns out that, when we run a hypothesis test with a significance threshold, the significance threshold is equal to the type I error (false positive) rate for the test. 
To see this, we can use a simulation.

Recall our quiz question example: the null hypothesis is that the probability of getting a quiz question correct is equal to 70%.
We’ll make a type I error if the null hypothesis is correct (the true probability of a correct answer is 70%), but we get a significant p-value anyways.

Now, consider the following simulation code:

false_positives = 0
sig_threshold = 0.05

for i in range(1000):
    sim_sample = np.random.choice(['correct', 'incorrect'], size=100, p=[0.7, 0.3])
    num_correct = np.sum(sim_sample == 'correct')
    p_val = binom_test(num_correct, 100, 0.7)
    if p_val < sig_threshold:
        false_positives += 1
        
print(false_positives/1000) #Output: 0.0512

This code does the following:

Set the significance threshold equal to 0.05 and a counter for false positives equal to zero.
Repeat these steps 1000 times:
Simulate 100 learners, where each learner has a 70% chance of answering a quiz question correctly.
Calculate the number of simulated learners who answered the question correctly. 
Note that, because each learner has a 70% chance of answering correctly, this number will likely be around 70, but will vary every time by random chance.
Run a binomial test for the simulated sample where the null hypothesis is that the probability of a correct answer is 70% (0.7).
Note that, every time we run this test, the null hypothesis is true because we simulated our data so that the probability of a correct answer is 70%.
Add 1 to our false positives counter every time we make a type I error (the p-value is significant). 
-Print the proportion of our 1000 tests (on simulated samples) that resulted in a false positive.
Note that the proportion of false positive tests is very similar to the value of the significance threshold (0.05).

Code:
import codecademylib3

# Import libraries
import numpy as np
from scipy.stats import binom_test

# Initialize num_errors
false_positives = 0
# Set significance threshold value
sig_threshold = 0.01

# Run binomial tests & record errors
for i in range(1000):
    sim_sample = np.random.choice(['correct', 'incorrect'], size=100, p=[0.8, 0.2])
    num_correct = np.sum(sim_sample == 'correct')
    p_val = binom_test(num_correct, 100, .8)
    if p_val < sig_threshold:
        false_positives += 1

# Print proportion of type I errors 
print(false_positives/1000)



Problems with Multiple Hypothesis Tests
While significance thresholds allow a data scientist to control the false positive rate for a single hypothesis test, 
this starts to break when performing multiple tests as part of a single study.

For example, suppose that we are writing a quiz at codecademy that is going to include 10 questions. 
For each question, we want to know whether the probability of a learner answering the question correctly is different from 70%. 
We now have to run 10 hypothesis tests, one for each question.

If the null hypothesis is true for every hypothesis test (the probability of a correct answer is 70% for every question) and we use a .05 significance level for each test, then:

When we run a hypothesis test for a single question, we have a 95% chance of getting the right answer (a p-value > 0.05) — and a 5% chance of making a type I error.

When we run hypothesis tests for two questions, we have only a 90% chance of getting the right answer for both hypothesis tests (.95*.95 = 0.90) 
— and a 10% chance of making at least one type I error.

When we run hypothesis tests for all 10 questions, we have a 60% chance of getting the right answer for all ten hypothesis tests (0.95^10 = 0.60) 
— and a 40% chance of making at least one type I error.

To address this problem, it is important to plan research out ahead of time: decide what questions you want to address and 
figure out how many hypothesis tests you need to run. When running multiple tests, use a lower significance threshold (eg., 0.01) 
for each test to reduce the probability of making a type I error.

Code:
import codecademylib3
import numpy as np
import matplotlib.pyplot as plt

# Set a significance threshold
sig_threshold = 0.1

# Define range of tests
num_tests = np.array(range(50))

# Calculate probability of at least one Type I error across multiple tests
probabilities = 1 - ((1 - sig_threshold) ** num_tests)

# Find the number of tests needed to reach a 50% Type I error rate
num_tests_50percent = np.where(probabilities >= 0.5)[0][0]

# Print the result to console
print(f"Number of tests needed for 50% probability of at least one Type I error: {num_tests_50percent}")

# Create the plot
plt.plot(num_tests, probabilities, label='Type I Error Probability')

# Add red dotted lines to indicate the 50% error rate intersection
plt.axhline(0.5, color='red', linestyle='--', label='50% Error Rate')
plt.axvline(num_tests_50percent, color='red', linestyle='--', label=f'{num_tests_50percent} Tests')

# Edit title and axis labels
plt.title('Type I Error Rate for Multiple Tests', fontsize=15)
plt.ylabel('Probability of at Least One Type I Error', fontsize=12)
plt.xlabel('Number of Tests', fontsize=12)

# Add a legend
plt.legend()

# Show the plot                
plt.show()



Significance Thresholds Review
Excellent work! In this lesson you have:

Learned to interpret a p-value using a significance threshold
Explored type I and type II errors in hypothesis testing
Shown that the significance threshold for a hypothesis test is equal to the type I error rate
Seen how multiple hypothesis tests lead to an increased probability of a type I error
In the workspace, we’ve borrowed a comic strip from xkcd that describes (with a little humor!) the problem of multiple hypothesis tests. 
The initial question asked in the comic is whether or not there is a link between jelly beans and acne. 
At first, scientists find no link — but then they run 20 more hypothesis tests looking for a link between specific colors of jelly beans and acne. 
One of those tests results in a significant p-value — and the final panel of the comic shows that this one test is the only test that gets published. 

The comic perfectly describes how running multiple hypothesis tests and only reporting significant results is extremely problematic. 
This has contributed to the reproducibility crisis in statistics. At a .05 significance level, approximately 5% of hypothesis tests will result in a false positive; 
but if we only report those false positives, we end up with a lot of published results that can’t be replicated.



Introduction to Linear Regression
Introduction to Linear Regression
Linear regression is a powerful modeling technique that can be used to understand the relationship between a quantitative variable and one or more other variables, 
sometimes with the goal of making predictions. 
For example, linear regression can help us answer questions like:

What is the relationship between apartment size and rental price for NYC apartments?
Is a mother’s height a good predictor of their child’s adult height?

The first step before fitting a linear regression model is exploratory data analysis and data visualization: is there a relationship that we can model? 

For example, suppose we collect heights (in cm) and weights (in kg) for 9 adults and inspect a plot of height vs. weight:

plt.scatter(data.height, data.weight)
plt.xlabel('height (cm)')
plt.ylabel('weight (kg)')
plt.show()

scatter plot showing a positive linear relationship between height and weight (people who are taller tend to weigh more)

When we look at this plot, we see that there is some evidence of a relationship between height and weight: people who are taller tend to weigh more. 
In the following exercises, we’ll learn how to model this relationship with a line. 
If you were to draw a line through these points to describe the relationship between height and weight, what line would you draw?

Code:
# Load libraries
import codecademylib3
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Read in the data
students = pd.read_csv('test_data.csv')

# Write equation for a line
y = 9.85 * students.hours_studied + 43

# Create the plot here: 
plt.scatter(students.hours_studied, students.score)
plt.xlabel('hours studied')
plt.ylabel('score')

plt.plot(students.hours_studied, y)
plt.show()



Equation of a Line
Like the name implies, LINEar regression involves fitting a line to a set of data points. 
In order to fit a line, it’s helpful to understand the equation for a line, which is often written as y=mx+b. In this equation:

x and y represent variables, such as height and weight or hours of studying and quiz scores.
b represents the y-intercept of the line. This is where the line intersects with the y-axis (a vertical line located at x = 0).
m represents the slope. This controls how steep the line is. 
If we choose any two points on a line, the slope is the ratio between the vertical and horizontal distance between those points;
this is often written as rise/run.

The following plot shows a line with the equation y = 2x + 12:

image showing a line with a point at the y-axis (a vertical line where the x-variable is equal to zero) labeled "y-intercept." 
The line also has two other points, which are connected by a horizontal and vertical dashed line, labeled "run" and "rise," respectively. 
The slope is calculated as rise/run which is equal to 2 in this example.

Note that we can also have a line with a negative slope. For example, the following plot shows the line with the equation y = -2x + 8:

Code:
# Load libraries
import codecademylib3
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Read in the data
students = pd.read_csv('test_data.csv')

# Write equation for a line
predicted_score = 10 * students.hours_studied + 45

# Create the plot
plt.scatter(students.hours_studied, students.score)
plt.plot(students.hours_studied, predicted_score)
plt.show()



Finding the "Best" Line
In the last exercise, we tried to eye-ball what the best-fit line might look like. In order to actually choose a line, we need to come up with some criteria for what “best” actually means.

Depending on our ultimate goals and data, we might choose different criteria; however, a common choice for linear regression is ordinary least squares (OLS). 
In simple OLS regression, we assume that the relationship between two variables x and y can be modeled as:

y=mx+b+error
We define “best” as the line that minimizes the total squared error for all data points. 
This total squared error is called the loss function in machine learning. 

For example, consider the following plot:

plot showing two points on either side of a line. One point is one unit below the line and has a label of -1; 
the other is 3 units above the line and has a label of 3

In this plot, we see two points on either side of a line. One of the points is one unit below the line (labeled -1). 
The other point is three units above the line (labeled 3). The total squared error (loss) is:


loss = (−1)^2 +(3)^2 = 1 + 9 = 10

Notice that we square each individual distance so that points below and above the line contribute equally to loss (when we square a negative number, the result is positive). 
To find the best-fit line, we need to find the slope and intercept of the line that minimizes loss.



Fitting a Linear Regression Model in Python
There are a number of Python libraries that can be used to fit a linear regression, 
but in this course, we will use the OLS.from_formula() function from statsmodels.api because it uses simple syntax and provides comprehensive model summaries.

Suppose we have a dataset named body_measurements with columns height and weight. 
If we want to fit a model that can predict weight based on height, we can create the model as follows:

model = sm.OLS.from_formula('weight ~ height', data = body_measurements)

We used the formula 'weight ~ height' because we want to predict weight (it is the outcome variable) using height as a predictor. Then, we can fit the model using .fit():

results = model.fit()

Finally, we can inspect a summary of the results using print(results.summary()). For now, we’ll only look at the coefficients using results.params, 
but the full summary table is useful because it contains other important diagnostic information.

print(results.params)

Output:

Intercept   -21.67
height        0.50
dtype: float64

This tells us that the best-fit intercept is -21.67, and the best-fit slope is 0.50.

Code:
# Load libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm

# Read in the data
students = pd.read_csv('test_data.csv')

# Create the model here:
model = sm.OLS.from_formula('score ~ hours_studied', data = students)

# Fit the model here:
results = model.fit()

# Print the coefficients here:
print(results.params)



Using a Regression Model for Prediction
Suppose that we have a dataset of heights and weights for 100 adults. We fit a linear regression and print the coefficients:

model = sm.OLS.from_formula('weight ~ height', data = body_measurements)
results = model.fit()
print(results.params)

Output:

Intercept   -21.67
height        0.50
dtype: float64

This regression allows us to predict the weight of an adult if we know their height. 
To make a prediction, we need to plug in the intercept and slope to our equation for a line. The equation is:

weight=0.50∗height−21.67
To make a prediction, we can plug in any height. For example, we can calculate that the expected weight for a 160cm tall person is 58.33kg:

weight=0.50∗160−21.67=58.33
In python, we can calculate this by plugging in values or by accessing the intercept and slope from results.params using their indices (0 and 1, respectively):

print(0.50 * 160 - 21.67) 
# Output: 58.33

# OR:

print(results.params[1]*160 + results.params[0])
# Output: 58.33

We can also do this calculation using the .predict() method on the fitted model. 
To predict the weight of a 160 cm tall person, we need to first create a new dataset with height equal to 160 as shown below:

newdata = {"height":[160]}
print(results.predict(newdata))

Output:

0      58.33
dtype: float64

Note that we get the same result (58.33) as with the other methods; however, it is returned as a data frame.



