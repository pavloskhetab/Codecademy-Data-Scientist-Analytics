Introduction to Hypothesis Testing (Simulating a One-Sample T-Test)
Learn about hypothesis testing by simulating a one-sample t-test

What is hypothesis testing?
Hypothesis Testing is a framework for asking questions about a dataset and answering them with probabilistic statements. 
There are many different kinds of hypothesis tests that can be used to address different kinds of questions and data. 
In this article, we’ll walk through a simulation of a one sample t- test in order to build intuition about how many different kinds of hypothesis tests work!

Step 1: Ask a Question
The International Baccalaureate Degree Programme (IBDP) is a world-wide educational program. 
Each year, students in participating schools can take a standardized assessment to earn their degree. 
Total scores on this assessment range from 1-45. Based on data provided here, the average total score for all students who took this exam in May 2020 was around 29.92. 
The distribution of scores looks something like this:

population_distribution

Imagine a hypothetical online school, Statistics Academy, that offers a 5-week test-preparation program. 
Suppose that 100 students who took the IBDP assessment in May 2020 were randomly 
chosen to participate in the first group of this program and that these 100 students earned an average score of 31.16 points on the exam — about 1.24 points higher than the international average! 
Are these students really outperforming their peers? Or could this difference be attributed to random chance?

Step 2: Define the Null and Alternative Hypotheses
Before attempting to answer this question, it is useful to reframe it in a way that is testable. 
Right now, our question (“Are the Statistics Academy students really outperforming their peers?”) is not clearly defined. 
Objectively, this group of 100 students performed better than the general population — but answering “yes, they outperformed their peers!” doesn’t feel particularly satisfying.

The reason it’s not satisfying is this: if we randomly choose ANY group of 100 students from the population of all test takers and calculate the average score for that sample, there’s a 50% chance it will be higher than the population average. Observing a higher average for a single sample is not surprising.

Of course, large differences from the population average are less probable — if all the Statistics Academy students earned 45 points on the exam (the highest possible score), 
we’d probably be convinced that these students had a real advantage. 
The trick is to quantify when differences are “large enough” to convince us that these students are systematically different from the general population. We can do this by reframing our question to focus on population(s), rather than our specific sample(s).

A hypothesis test begins with two competing hypotheses about the population that a particular sample comes from — in this case, the 100 Statistics Academy students:

Hypothesis 1 (technically called the Null Hypothesis): The 100 Statistics Academy students are a random sample from the general population of test takers, who had an average score of 29.92. 
If this hypothesis is true, the Statistics Academy students earned a slightly higher average score by random chance. Visually, this set-up looks something like this:
null hypothesis
Hypothesis 2 (technically called the Alternative Hypothesis): The 100 Statistics Academy students came from a population with an average score that is different from 29.92. 
In this hypothesis, we need to imagine two different populations that don’t actually exist: one where all students took the Statistics Academy test-prep program and one where none of the students took the program. 
If the alternative hypothesis is true, our sample of 100 Statistics Academy students came from a different population than the other test-takers. This can be visualized as follows:
alternative hypothesis
There’s one more clarification we need to make in order to fully specify the alternative hypothesis. 
Notice that, so far, we have not said anything about the average score for “population 1” in the diagram above, other than that it is NOT 29.92. 
We actually have three choices for what the alternative hypothesis says about this population average:

it is GREATER THAN 29.92
it is NOT EQUAL TO (i.e., GREATER THAN OR LESS THAN) 29.92
it is LESS THAN 29.92
Later on, we’ll discuss how the choice between “greater than”, “not equal to” and “less than” impacts the test.

Fill in the blank
Questions
Suppose that a random sample of 300 runners in the Boston Marathon were chosen to run in a new model of shoes: the Flying Flier. 
The average finishing time for these 300 runners was 230 minutes, while the average finishing time for all runners was 233 minutes. 
Was the average finishing time for this sample significantly different from the average finishing time for all runners?

Fill in the blanks to indicate the null and alternative hypotheses for the one-sample t-test to address this question.

Code
Null Hypothesis: The average finishing time for Boston Marathon runners wearing the Flying Flyer is blank 1 blank 2 minutes.

Alternative Hypothesis: The average finishing time for Boston Marathon runners wearing the Flying Flyer is blank 3 blank 4 minutes.
Answer Choices
233
not equal to
equal to
233
230
230
Click or drag and drop to fill in the blank

Step 3: Determine the Null Distribution
Now that we have our null hypothesis, we can generate a null distribution: the distribution (across repeated samples) of the statistic we are interested in if the null hypothesis is true. 
In the IBDP example described above, this is the distribution of average scores for repeated samples of size 100 drawn from a population with an average score of 29.92.

This might feel familiar if you’ve learned about the central limit theorem (CLT)! Statistical theory allows us to estimate the shape of this distribution using a single sample. 
You can learn how to do this by reading more about the CLT, but for the purposes of this example, let’s simulate the null distribution using our population. We can do this by:

Taking many random samples of 100 scores, each, from the population
Calculating and storing the average score for each of those samples
Plotting a histogram of the average scores
This yields the following distribution, which is approximately normal and centered at the population average:

sampling_distribution

If the null hypothesis is true, then the average score of 31.16 observed among Statistics Academy students is simply one of the values from this distribution. 
Let’s plot the sample average on the null distribution. Notice that this value is within the range of the null distribution, but it is off to the right where there is less density:

population_distribution_with_sample_mean

Step 4: Calculate a P-Value (or Confidence Interval)
Here is the basic question asked in our hypothesis test:

Given that the null hypothesis is true (that the 100 students from Statistics Academy were sampled from a population with an average IBDP score of 29.92), how likely is it that their average score is 31.16?

The minor problem with this question is that the probability of any exact average score is very small, so we really want to estimate the probability of a range of scores. Let’s now return to our three possible alternative hypotheses and see how the question and calculation each change slightly, depending on which one we choose:

Option 1
Alternative Hypothesis: The sample of 100 scores earned by Statistics Academy students came from a population with an average score that is greater than 29.92.

In this case, we want to know the probability of observing a sample average greater than or equal to 31.16 given that the null hypothesis is true. 
Visually, this is the proportion of the null distribution that is greater than or equal to 31.16 (shaded in red below). Here, the red region makes up about 3.1% of the total distribution. 
This proportion, which is usually written in decimal form (i.e., 0.031), is called a p-value.

one_sided_upper_test

Option 2
Alternative Hypothesis: The sample of 100 scores earned by Statistics Academy students came from a population with an average score that is not equal to (i.e., greater than OR less than) 29.92.

We observed a sample average of 31.16 among the Statistics Academy students, which is 1.24 points higher than the population average (if the null hypothesis is true) of 29.92. 
In the first version of the hypothesis test (option 1), we estimated the probability of observing a sample average that is at least 1.24 points higher than the population average. 
For the alternative hypothesis described in option 2, we’re interested in the probability of observing a sample average that is at least 1.24 points DIFFERENT (higher OR lower) than the population average. 
Visually, this is the proportion of the null distribution that is at least 1.24 units away from the population average (shaded in red below). 
Note that this area is twice as large as in the previous example, leading to a p-value that is also twice as large: 0.062.

two_sided_test

While option 1 is often referred to as a One Sided or One-Tailed test, this version is referred to as a Two Sided or Two-Tailed test, referencing the two tails of the null distribution that are counted in the p-value. 
It is important to note that most hypothesis testing functions in Python and R implement a two-tailed test by default.

Option 3
Alternative Hypothesis: The sample of 100 scores earned by Statistics Academy students came from a population with an average score that is less than 29.92.

Here, we want to know the probability of observing a sample average less than or equal to 31.16, given that the null hypothesis is true. This is also a one-sided test, just the opposite side from option 1. 
Visually, this is the proportion of the null distribution that is less than or equal to 31.16. This is a very large area (almost 97% of the distribution!), leading to a p-value of 0.969.

lower_tail_test

At this point, you may be thinking: why would anyone ever choose this version of the alternative hypothesis?! In real life, if a test-prep program was planning to run a rigorous experiment to see whether their students are scoring differently than the general population, they should choose the alternative hypothesis before collecting any data. At that point, they won’t know whether their sample of students will have an average score that is higher or lower than the population average — although they probably hope that it is higher.

Step 5: Interpret the Results
In the three examples above, we calculated three different p-values (0.031, 0.062, and 0.969, respectively). Consider the first p-value of 0.031. The interpretation of this number is as follows:

If the 100 students at Statistics Academy were randomly selected from the full population (which had an average score of 29.92), there is a 3.1% chance of their average score being 31.16 points or higher.

This means that it is relatively unlikely, but not impossible, that the Statistics Academy students scored higher (on average) than their peers by random chance, despite no real difference at the population level. In other words, the observed data is unlikely if the null hypothesis is true. Note that we have directly tested the null hypothesis, but not the alternative hypothesis! We therefore need to be a little careful about how we interpret this test: we cannot say that we’ve proved that the alternative hypothesis is the truth — only that the data we collected would be unlikely under null hypothesis, and therefore we believe that the alternative hypothesis is more consistent with our observations.

Fill in the blank
Questions
Suppose that a random sample of 300 runners in the Boston Marathon were chosen to run in a new model of shoes: the Flying Flier. 
The average finishing time for these 300 runners was 230 minutes, while the average finishing time for all runners was 233 minutes. 
We ran a 2-sided one-sample t-test with the following null and alternative hypotheses:

Null Hypothesis: The average finishing time for Boston Marathon runners wearing the Flying Flyer is equal to 233 minutes.

Alternative Hypothesis: The average finishing time for Boston Marathon runners wearing the Flying Flyer is not equal to 233 minutes

If we get a p-value of 0.10 for this test, fill in the blanks to provide a correct interpretation of that number.

Code
If the 300 runners were randomly selected from the full population of runners (who had an average finishing time of 233 minutes), 
there is a blank 1 chance of their average finishing time being at least 3 minutes blank 2 the average finishing time of all runners.
Answer Choices
different from
faster than
10%
slower than
.10%
Click or drag and drop to fill in the blank

Significance thresholds
While it is perfectly reasonable to report a p-value, many data scientists use a predetermined threshold to decide whether a particular p-value is significant or not.
P-values below the chosen threshold are declared significant and lead the data scientist to “reject the null hypothesis in favor of the alternative”. 
A common choice for this threshold, which is also sometimes referred to as Alpha, is 0.05 — but this is an arbitrary choice! 
Using a lower threshold means you are less likely to find significant results, but also less likely to mistakenly report a significant result when there is none.

Using the first p-value of 0.031 and a significance threshold of 0.05, 
Statistics Academy could reject the null hypothesis and conclude that the 100 students who participated in their program scored significantly higher on the test than the general population.

Impact of the Alternative Hypothesis
Note that different alternative hypotheses can lead to different conclusions. 
For example, the one-sided test described above (p = 0.031) would lead a data scientist to reject the null at a 0.05 significance level. 
Meanwhile, a two-sided test on the same data leads to a p-value of 0.062, which is greater than the 0.05 threshold. 
Thus, for the two-sided test, a data scientist could not reject the null hypothesis. This highlights the importance of choosing an alternative hypothesis ahead of time!

Summary and further applications
As you have hopefully learned from this article, hypothesis testing can be a useful framework for asking and answering questions about data. 
Before you use hypothesis tests in practice, it is important to remember the following:

A p-value is a probability, usually reported as a decimal between zero and one.
A small p-value means that an observed sample statistic (or something more extreme) would be unlikely to occur if the null hypothesis is true.
A significance threshold can be used to translate a p-value into a “significant” or “non-significant” result.
In practice, the alternative hypothesis and significance threshold should be chosen prior to data collection.
There are many different hypothesis tests that can be used to answer different kinds of questions about data. 
Now that you’ve learned about one, you have all the skills you’ll need to understand many more!



One-Sample T-Tests in SciPy
Introduction
In this lesson, we’ll walk through the implementation of a one-sample t-test in Python. One-sample t-tests are used for comparing a sample average to a hypothetical population average. 
For example, a one-sample t-test might be used to address questions such as:

Is the average amount of time that visitors spend on a website different from 5 minutes?
Is the average amount of money that customers spend on a purchase more than 10 USD?
As an example, let’s imagine the fictional business BuyPie, which sends ingredients for pies to your household so that you can make them from scratch. 
Suppose that a product manager wants online BuyPie orders to cost around 1000 Rupees on average. 
In the past day, 50 people made an online purchase and the average payment per order was less than 1000 Rupees. 
Are people really spending less than 1000 Rupees on average? Or is this the result of chance and a small sample size?

Code:
import numpy as np

prices = np.genfromtxt("prices.csv")

# print prices:
print(prices)

# calculate prices_mean and print it out:
prices_mean = np.mean(prices)
print(prices_mean)



Implementing a One-Sample T-Test
In the last exercise, we inspected a sample of 50 purchase prices at BuyPie and saw that the average was 980 Rupees. 
Suppose that we want to run a one-sample t-test with the following null and alternative hypotheses:

Null: The average cost of a BuyPie order is 1000 Rupees
Alternative: The average cost of a BuyPie order is not 1000 Rupees.
SciPy has a function called ttest_1samp(), which performs a one-sample t-test for you. ttest_1samp() requires two inputs, a sample distribution (eg. the list of the 50 observed purchase prices) 
and a mean to test against (eg. 1000):

tstat, pval = ttest_1samp(sample_distribution, expected_mean)

The function uses your sample distribution to determine the sample size and estimate the amount of variation in the population — which are used to estimate the null distribution. 
It returns two outputs: the t-statistic (which we won’t cover in this course), and the p-value.



from scipy.stats import ttest_1samp
import numpy as np

prices = np.genfromtxt("prices.csv")
print(prices)

prices_mean = np.mean(prices)
print("mean of prices: " + str(prices_mean))

# use ttest_1samp to calculate pval
tstat, pval = ttest_1samp(prices, 1000)
# print pval
print(pval)



Assumptions of a One Sample T-Test
When running any hypothesis test, it is important to know and verify the assumptions of the test. The assumptions of a one-sample t-test are as follows:

The sample was randomly selected from the population
For example, if you only collect data for site visitors who agree to share their personal information, this subset of visitors was not randomly selected and may differ from the larger population.

The individual observations were independent
For example, if one visitor to BuyPie loves the apple pie they bought so much that they convinced their friend to buy one too, those observations were not independent.

The data is normally distributed without outliers OR the sample size is large (enough)
There are no set rules on what a “large enough” sample size is, but a common threshold is around 40. 
For sample sizes smaller than 40, and really all samples in general, it’s a good idea to make sure to plot a histogram of your data and check for outliers, 
multi-modal distributions (with multiple humps), or skewed distributions. If you see any of those things for a small sample, a t-test is probably not appropriate.

In general, if you run an experiment that violates (or possibly violates) one of these assumptions, you can still run the test and report the results — 
but you should also report assumptions that were not met and acknowledge that the test results could be flawed.

Code:
import codecademylib3
import numpy as np
import matplotlib.pyplot as plt

prices = np.genfromtxt("prices.csv")

#plot your histogram here
plt.hist(prices)
plt.show()



Review
Congratulations! You now know how to implement a one-sample t-test in Python and verify the assumptions of the test. To recap, here are some of the things you learned:

One-sample t-tests are used for comparing a sample mean to an expected population mean
A one-sample t-test can be implemented in Python using the SciPy ttest_1samp() function
Assumptions of a one-sample t-test include:
The sample was randomly drawn from the population of interest
The observations in the sample are independent
The sample size is large “enough” or the sample data is normally distributed
Instructions
As a final exercise, some data has been loaded for you with purchase prices for consecutive days at BuyPie.
You can access the first day using daily_prices[0], the second using daily_prices[1], etc.. 
To practice running a one-sample t-test and inspecting the resulting p-value, try the following:

Calculate and print out a p-value for day 1 where the null hypothesis is that the average purchase price was 1000 Rupees and the alternative hypothesis is that the average purchase price was not 1000 Rupees. 
Print out the p-value.

Run the same hypothesis tests for days 1-10 (the fastest way to do this is with a for-loop!) and print out the resulting p-values. 
What’s the smallest p-value you observe for those 10 days?

Try changing the null hypothesis so that the expected population mean that you’re testing against is different from 1000. 
Try any numbers that you want. How do your p-values change?

Solution code can be found in solution.py

Code:
from scipy.stats import ttest_1samp, shapiro
import numpy as np
import matplotlib.pyplot as plt

# Load daily prices data
daily_prices = np.genfromtxt("daily_prices.csv", delimiter=",")

# Print the prices for Day 1 to verify the data
print('Prices for Day 1:', daily_prices[0])

# Create a histogram to visualize values in the dataset for Day 1
plt.hist(daily_prices[0], edgecolor='black', bins=20)
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.title('Histogram of Daily Prices for Day 1')
plt.show()

# Perform Shapiro-Wilk test for normality on each dataset
for x in range(10):
    stat, p_value = shapiro(daily_prices[x])
    print(f'Day {x + 1}: Shapiro-Wilk Test Statistic = {stat}, p-value = {p_value}')

    # Interpret the Shapiro-Wilk test results for each dataset
    if p_value < 0.05:
        print(f"Day {x + 1}: The data does not follow a normal distribution.\n")
    else:
        print(f"Day {x + 1}: The data appears to follow a normal distribution.\n")

# Task 1 - Calculate and print p-value for day 1
# H0: average price was 1000 rupees and 
# Ha: average price != 1000 rupees.

# Counting the sample size for day 1
print('Sample Size for Day 1:', np.count_nonzero(daily_prices[0]))

# Calculate the mean for day 1
daily_prices_mean_day_1 = np.mean(daily_prices[0])
print('Mean for Day 1:', daily_prices_mean_day_1)

# Perform the one-sample t-test
tstat, p_value = ttest_1samp(daily_prices[0], 1000)

# Print the p-value
print('P-value:', p_value)

# Interpret the p-value result
if p_value < 0.05:
    print("The average daily price is significantly different from the population mean of 1000 Rupees.")
else:
    print("There is no significant difference in the average daily price compared to 1000 Rupees.")

# Task 2: Run the same hypothesis tests for days 1-10 and print out the resulting p-values.
p_values = []
for x in range(10):
    tstat, p_value = ttest_1samp(daily_prices[x], 1000)
    p_values.append(p_value)
    print(f"Day {x + 1}: p-value = {p_value}")

# Find the smallest p-value among the 10 days
min_p_value = min(p_values)
print(f"The smallest p-value observed for days 1-10 is: {min_p_value}")

# Task 3: Change the null hypothesis so that the expected population mean is different from 1000.
print("Day 1-10 with a different null hypothesis (H0 = 1100):")
for x in range(10):
    tstat, pval = ttest_1samp(daily_prices[x], 1100)
    print(f"Day {x + 1}: p-value = {pval}")  # Use pval instead of p_value




--Simulating a Binomial Test--

Introduction
In this lesson, we will walk through a simulation of a binomial hypothesis test in Python. 
Binomial tests are useful for comparing the frequency of some outcome in a sample to the expected probability of that outcome. 
For example, if we expect 90% of ticketed passengers to show up for their flight but only 80 of 100 ticketed passengers actually show up, 
we could use a binomial test to understand whether 80 is significantly different from 90.

Binomial tests are similar to one-sample t-tests in that they test a sample statistic against some population-level expectation. The difference is that:

binomial tests are used for binary categorical data to compare a sample frequency to an expected population-level probability
one-sample t-tests are used for quantitative data to compare a sample mean to an expected population mean.
In Python, as in many other programming languages used for statistical computing, 
there are a number of libraries and functions that allow a data scientist to run a hypothesis test in a single line of code. 
However, a data scientist will be much more likely to spot and fix potential errors and interpret results correctly 
if they have a conceptual understanding of how these functions work. To that end, this lesson will help you build your own conceptual understanding!

Code:
import pandas as pd
import codecademylib3

monthly_report = pd.read_csv('monthly_report.csv')

print(monthly_report.head())



Summarizing the Sample
The marketing department at Live-it-LIVE reports that, during this time of year, about 10% of visitors to Live-it-LIVE.com make a purchase.

The monthly report shows every visitor to the site and whether or not they made a purchase. The checkout page had a small bug this month, 
so the business department wants to know whether the purchase rate dipped below expectation. They’ve asked us to investigate this question.

In order to run a hypothesis test to address this, we’ll first need to know two things from the data:

The number of people who visited the website
The number of people who made a purchase on the website
Assuming each row of our dataset represents a unique site visitor, we can calculate the number of people who visited the website by finding the number of rows in the data frame. 
We can then find the number who made a purchase by using a conditional statement to add up the total number of rows where a purchase was made.

For example, suppose that the dataset candy contains a column named chocolate with 'yes' recorded for every candy that has chocolate in it and 'no' otherwise. 
The following code calculates the sample size (the number of candies) and the number of those candies that contain chocolate:

## sample size (number of rows): 
samp_size = len(candy)
 
## number with chocolate: 
total_with_chocolate = np.sum(candy.chocolate == 'yes')

Code:
import numpy as np
import pandas as pd
import codecademylib3

monthly_report = pd.read_csv('monthly_report.csv')

#print the head of monthly_report:
print(monthly_report.head())

#calculate and print sample_size:
sample_size = len(monthly_report)
print('sample size:')
print(sample_size)

#calculate and print num_purchased:
num_purchased = np.sum(monthly_report.purchase == 'y')
print("number of purchases:")
print(num_purchased)



Simulating Randomness
In the last exercise, we calculated that there were 500 site visitors to live-it-LIVE.com this month and 41 of them made a purchase. 
In comparison, if each of the 500 visitors had a 10% chance of making a purchase, we would expect around 50 of those visitors to buy something. 
Is 41 different enough from 50 that we should question whether this months’ site visitors really had a 10% chance of making a purchase?

To conceptualize why our expectation (50) and observation (41) might not be equal — EVEN IF there was no dip in the purchase probability — let’s turn to a common probability example: flipping a fair coin. 
We can simulate a coin flip in Python using the numpy.random.choice() function:

flip = np.random.choice(['heads', 'tails'], size=1, p=[0.5, 0.5])
print(flip) 
## output is either ['heads'] or ['tails']

If we run this code (or flip a real coin) a few times, we’ll find that — just like we can’t know ahead of time whether any single visitor to Live-it-LIVE.com 
will make a purchase — we can’t predict the outcome of any individual coin flip.

If we flip a fair coin 10 times in a row, we expect about 5 of those coins to come up heads (50%). We can simulate this in python by changing the size parameter of numpy.random.choice():

flip = np.random.choice(['heads', 'tails'], size=10, p=[0.5, 0.5])
print(flip)
## output is something like: ['heads' 'heads' 'heads' 'tails' 'tails' 'heads' 'heads' 'tails' 'heads' 'heads']

If you try this yourself, it’s perfectly reasonable that you’ll get only four heads, or maybe six or seven! Because this is a random process, 
we can’t guarantee that exactly half of our coin flips will come up heads. Similarly, even if each Live-it-LIVE visitor has a 10% chance of making a purchase, 
that doesn’t mean we expect exactly 10% to do so in any given sample.

Code:
import numpy as np
import pandas as pd

monthly_report = pd.read_csv('monthly_report.csv')

#simulate one visitor:
one_visitor = np.random.choice(['y','n'], size=1, p=[0.1,0.9])
print(one_visitor)

#simulate 500 visitors:
simulated_monthly_visitors = np.random.choice(['y','n'], size=500, p=[0.1,0.9])
print(simulated_monthly_visitors)



Simulating the Null Distribution I
The first step in running a hypothesis test is to form a null hypothesis. For the question of whether the purchase rate at Live-it-LIVE.com was different from 10% this month, 
the null hypothesis describes a world in which the true probability of a visitor making a purchase was exactly 10%, but by random chance, we observed that only 41 visitors (8.2%) made a purchase.

Let’s return to the coin flip example from the previous exercise. We can simulate 10 coin flips and print out the number of those flips that came up heads using the following code:

flips = np.random.choice(['heads', 'tails'], size=10, p=[0.5, 0.5])
num_heads = np.sum(flips == 'heads')
print(num_heads)
## output: 4

If we run this code a few times, we’ll likely see different results each time. This will give us get a sense for the range in the number of heads that could occur by random chance, 
even if the coin is fair. We’re more likely to see numbers like four, five, or six, but maybe we’ll see something more extreme every once in a while — ten heads in a row, or even zero!

Code:
import numpy as np
import pandas as pd

monthly_report = pd.read_csv('monthly_report.csv')

#simulate 500 visitors:
simulated_monthly_visitors = np.random.choice(['y', 'n'], size=500, p=[0.1, 0.9])

#calculate the number of simulated visitors who made a purchase:
num_purchased = np.sum(simulated_monthly_visitors == 'y')

print(num_purchased)

#Initial run: 53, After 5 runs: 45, 47, 55, 44, 54. Furthest number from 50 is 55 



Simulating the Null Distribution II
In the last exercise, we simulated a random sample of 500 visitors, where each visitor had a 10% chance of making a purchase. 
When we pressed “Run” a few times, we saw that the number of purchases varied from sample to sample, but was around 50.

Similarly, we simulated a single random sample of 10 coin flips, where each flip had a 50% chance of coming up heads. 
We saw that the number of simulated heads was not necessarily 5, but somewhere around 5.

By running the same simulated experiment many times, we can get a sense for how much a particular outcome (like the number of purchases, or heads) varies by random chance. 
Consider the following code:

outcomes = []
for i in range(10000): 
    flips = np.random.choice(['heads', 'tails'], size=10, p=[0.5, 0.5])
    num_heads = np.sum(flips == 'heads')
    outcomes.append(num_heads)
print(outcomes)
## output is something like: [3, 4, 5, 8, 5, 6, 4, 5, 3, 2, 8, 5, 7, 4, 4, 5, 4, 3, 6, 5,...]

In this code chunk, we’ve done the following:

initialized an empty list named outcomes to store the number of ‘heads’ from simulated samples of coin flips
set up a for-loop to repeat the steps below 10000 times:
flip a fair coin 10 times
calculate the number of those 10 flips that came up heads
append that number onto outcomes
Note that 10000 is an arbitrarily chosen large number — it’s big enough that it will yield almost all possible outcomes of our experiment, 
and small enough that the simulation still runs quickly. 

From inspecting the output, we can see that the number of ‘heads’ varied between 0 and 10:

min_heads = np.min(outcomes) 
print(min_heads) #output: 0
 
max_heads = np.max(outcomes)
print(max_heads) #output: 10

Thus, if we flip a fair coin 10 times, we could observe anywhere between 0 and 10 heads by random chance.

Code:
import numpy as np
import pandas as pd

null_outcomes = []

#start for loop here:
for i in range(10000):
  
  simulated_monthly_visitors = np.random.choice(['y', 'n'], size=500, p=[0.1, 0.9])

  num_purchased = np.sum(simulated_monthly_visitors == 'y')

  null_outcomes.append(num_purchased) 


#calculate the minimum and maximum values in null_outcomes here:
null_min = np.min(null_outcomes)
print('Min value for null outcomes', null_min)

null_max = np.max(null_outcomes)
print('Max value for null outcomes',null_max)



Inspecting the Null Distribution
In the previous exercise, we simulated 10000 different samples of 500 visitors, where each visitor had a 10% chance of making a purchase, and calculated the number of purchases per sample. 
Upon further inspection, we saw that those numbers ranged from around 25 to 75. This is useful information, but we can learn even more from inspecting the full distribution.

For example, recall our 10000 coin flip experiments: for each experiment, we flipped a fair coin 10 times and recorded the number of heads in a list named outcomes. 
We can plot a histogram of outcomes using matplotlib.pyplot.hist(). We can also add a vertical line at any x-value using matplotlib.pyplot.axvline():

import matplotlib.pyplot as plt
plt.hist(outcomes)
plt.axvline(2, color = 'r')
plt.show()

Output:

distribution of coin flips

This histogram shows us that, over 10000 experiments, we observed as few as 0 and as many as 10 heads out of 10 flips. 
However, we were most likely to observe around 4-6 heads. It would be unlikely to observe only 2 heads (where the vertical red line is).

Code:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import codecademylib3

null_outcomes = []

for i in range(10000):
  simulated_monthly_visitors = np.random.choice(['y', 'n'], size=500, p=[0.1, 0.9])

  num_purchased = np.sum(simulated_monthly_visitors == 'y')

  null_outcomes.append(num_purchased)

#plot the histogram here:
plt.hist(null_outcomes)
plt.axvline(41, color = 'r')
plt.show()
plt.close()



Confidence Intervals
So far, we’ve inspected the null distribution and calculated the minimum and maximum values. 
While the number of purchases in each simulated sample ranged roughly from 25 to 75 by random chance, upon further inspection of the distribution, we saw that those extreme values happened very rarely.

By reporting an interval covering 95% of the values instead of the full range, we can say something like: “we are 95% confident that, if each visitor has a 10% chance of making a purchase, 
a random sample of 500 visitors will make between 37 and 63 purchases.” We can use the np.percentile() function to calculate this 95% interval as follows:

np.percentile(outcomes, [2.5,97.5])
# output: [37. 63.]

We calculated the 2.5th and 97.5th percentiles so that exactly 5% of the data falls outside those percentiles (2.5% above the 97.5th percentile, and 2.5% below the 2.5th percentile). 
This leaves us with a range covering 95% of the data.

If our observed statistic falls outside this interval, then we can conclude it is unlikely that the null hypothesis is true. 
In this example, because 41 falls within the 95% interval (37 - 63), it is still reasonably likely that we observed a lower purchase rate by random chance, even though the null hypothesis was true.

Code:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import codecademylib3

null_outcomes = []

for i in range(10000):
  simulated_monthly_visitors = np.random.choice(['y', 'n'], size=500, p=[0.1, 0.9])

  num_purchased = np.sum(simulated_monthly_visitors == 'y')

  null_outcomes.append(num_purchased)

#calculate the 90% interval here:
null_90CI = np.percentile(null_outcomes, [5,95])
print(null_90CI)



Calculating a One-Sided P-Value
P-value calculations and interpretations depend on the alternative hypothesis of a test, a description of the difference from expectation that we are interested in.

For example, let’s return to the 10-coin-flip example from earlier. Suppose that we flipped a coin 10 times and observed only 2 heads. 
We might run a hypothesis test with the following null and alternative hypotheses:

Null: the probability of heads is 0.5
Alternative: the probability of heads is less than 0.5
This hypothesis test asks the question: IF the probability of heads is 0.5, what’s the probability of observing 2 or fewer heads among a single sample of 10 coin flips?

Earlier, we used a for-loop to repeatedly (10000 times!) flip a fair coin 10 times, and store the number of heads (for each set of 10 flips) in a list named outcomes. 
The probability of observing 2 or fewer heads among 10 coin flips is approximately equal to the proportion of those 10000 experiments where we observed 0, 1, or 2 heads:

import numpy as np
outcomes = np.array(outcomes)
p_value = np.sum(outcomes <= 2)/len(outcomes) 
print(p_value) #output: 0.059

This calculation is equivalent to calculating the proportion of this histogram that is colored in red:null distribution with bars colored red for values less than or equal to 2

We estimated that the probability of observing 2 or fewer heads is about 0.059 (5.9%). This probability (0.059) is referred to as a one-sided p-value.

Code:
